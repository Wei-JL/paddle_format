"""
VOCæ•°æ®é›†å¤„ç†ç±»

ç”¨äºå¤„ç†Pascal VOCæ ¼å¼çš„æ•°æ®é›†ï¼Œæä¾›ï¼š
- æ•°æ®é›†åŸºæœ¬éªŒè¯
- å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶åŒ¹é…
- ç©ºæ ‡æ³¨æ–‡ä»¶æ£€æµ‹å’Œåˆ é™¤
- æ•°æ®é›†åˆ’åˆ†åŠŸèƒ½
- ç±»åˆ«æå–åŠŸèƒ½
- ç±»åˆ«è¿‡æ»¤åŠŸèƒ½
- å›¾åƒå°ºå¯¸æ£€æŸ¥å’Œä¿®æ­£åŠŸèƒ½
- å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
"""

import os
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Tuple, Set, Dict
import sys
import random
import shutil
import cv2
from tqdm import tqdm
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from xml.dom import minidom

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ - ä½¿ç”¨å…¨å±€å˜é‡
sys.path.append(str(Path(__file__).parent.parent))
from logger_code.logger_sys import get_logger
from global_var.global_cls import *

# è·å–å½“å‰æ–‡ä»¶åä½œä¸ºæ—¥å¿—æ ‡è¯†
current_filename = Path(__file__).stem
logger = get_logger(current_filename)

class VOCDataset:
    """VOCæ•°æ®é›†å¤„ç†ç±»"""
    
    def __init__(self, dataset_path: str, user_labels_file: str = None, 
                 train_ratio: float = TRAIN_RATIO, 
                 val_ratio: float = VAL_RATIO, test_ratio: float = TEST_RATIO,
                 max_workers: int = 4, annotations_folder_name: str = ANNOTATIONS_DIR):
        """
        åˆå§‹åŒ–VOCæ•°æ®é›†
        
        Args:
            dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
            user_labels_file: ç”¨æˆ·æä¾›çš„æ­£ç¡®æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            max_workers: çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            annotations_folder_name: æ ‡æ³¨æ–‡ä»¶å¤¹åç§°ï¼Œé»˜è®¤ä¸º"Annotations"
        """
        self.dataset_path = Path(dataset_path)
        # è‡ªåŠ¨è·å–æ•°æ®é›†åç§°ï¼ˆæ–‡ä»¶å¤¹æœ€åä¸€ä¸ªåç§°ï¼‰
        self.dataset_name = self.dataset_path.name
        
        # æ ‡æ³¨æ–‡ä»¶å¤¹åç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
        self.annotations_folder_name = annotations_folder_name
        
        # æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        
        # çº¿ç¨‹æ± é…ç½®
        self.max_workers = max_workers
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        self._lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨é”
        
        # æ ‡å‡†VOCç›®å½•ç»“æ„ - ä½¿ç”¨os.path.joinæ‹¼æ¥è·¯å¾„
        self.annotations_dir = Path(os.path.join(str(self.dataset_path), self.annotations_folder_name))
        self.images_dir = Path(os.path.join(str(self.dataset_path), JPEGS_DIR))
        self.imagesets_dir = Path(os.path.join(str(self.dataset_path), IMAGESETS_DIR, MAIN_DIR))
        
        # æ¸…æ´—åçš„è¾“å‡ºç›®å½• - ä½¿ç”¨os.path.joinæ‹¼æ¥è·¯å¾„
        self.annotations_output_dir = Path(os.path.join(str(self.dataset_path), "Annotations_clear"))
        
        # ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶
        self.user_labels_file = user_labels_file
        self.user_labels = set()  # ç”¨æˆ·æä¾›çš„æ­£ç¡®æ ‡ç­¾é›†åˆ
        
        # è®°å½•ç¼ºå¤±æ–‡ä»¶çš„åˆ—è¡¨
        self.missing_xml_files = []  # ç¼ºå°‘XMLæ–‡ä»¶çš„å›¾åƒ
        self.missing_image_files = []  # ç¼ºå°‘å›¾åƒæ–‡ä»¶çš„XML
        self.images_without_xml = []  # æœ‰å›¾åƒä½†æ²¡æœ‰XMLçš„æ–‡ä»¶åˆ—è¡¨
        
        # æœ‰æ•ˆçš„æ–‡ä»¶å¯¹åˆ—è¡¨
        self.valid_pairs = []
        
        # ç±»åˆ«é›†åˆ
        self.classes = set()
        
        # å°ºå¯¸ä¸åŒ¹é…è®°å½•
        self.dimension_mismatches = []
        self.channel_mismatches = []
        
        logger.info(f"åˆå§‹åŒ–VOCæ•°æ®é›†: {self.dataset_name}")
        logger.info(f"æ•°æ®é›†è·¯å¾„: {self.dataset_path.absolute()}")
        logger.info(f"åˆ’åˆ†æ¯”ä¾‹ - è®­ç»ƒé›†: {self.train_ratio}, éªŒè¯é›†: {self.val_ratio}, æµ‹è¯•é›†: {self.test_ratio}")
        logger.info(f"çº¿ç¨‹æ± é…ç½® - æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        
        # éªŒè¯ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶
        if self.user_labels_file:
            self._validate_user_labels()
        
        print(f"âœ… VOCæ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {self.dataset_name}")
        print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {self.dataset_path}")
        print(f"ğŸ§µ çº¿ç¨‹æ± é…ç½®: {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        print("ğŸ’¡ è¯·è°ƒç”¨ one_click_complete_conversion() æ–¹æ³•å¼€å§‹æ•°æ®å¤„ç†")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿çº¿ç¨‹æ± æ­£ç¡®å…³é—­"""
        if hasattr(self, 'thread_pool'):
            self.thread_pool.shutdown(wait=True)
    
    def _validate_user_labels(self):
        """éªŒè¯ç”¨æˆ·æä¾›çš„æ ‡ç­¾æ–‡ä»¶"""
        logger.info("éªŒè¯ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶...")
        
        if not self.user_labels_file:
            logger.warning("æœªæä¾›ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶")
            return
        
        labels_path = Path(self.user_labels_file)
        if not labels_path.exists():
            logger.error(f"ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {labels_path}")
            raise FileNotFoundError(f"ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {labels_path}")
        
        try:
            with open(labels_path, 'r', encoding=DEFAULT_ENCODING) as f:
                for line in f:
                    label = line.strip()
                    if label:
                        self.user_labels.add(label)
            
            logger.info(f"åŠ è½½ç”¨æˆ·æ ‡ç­¾: {len(self.user_labels)} ä¸ª")
            logger.info(f"æ ‡ç­¾åˆ—è¡¨: {sorted(self.user_labels)}")
            
        except Exception as e:
            logger.error(f"è¯»å–ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _validate_basic_structure(self):
        """éªŒè¯æ•°æ®é›†åŸºæœ¬ç»“æ„"""
        logger.info("éªŒè¯æ•°æ®é›†åŸºæœ¬ç»“æ„...")
        
        # æ£€æŸ¥å¿…éœ€çš„ç›®å½•
        required_dirs = [
            (self.annotations_dir, "æ ‡æ³¨ç›®å½•"),
            (self.images_dir, "å›¾åƒç›®å½•")
        ]
        
        for dir_path, dir_name in required_dirs:
            if not dir_path.exists():
                error_msg = f"{dir_name}ä¸å­˜åœ¨: {dir_path}"
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)
            
            logger.debug(f"ç›®å½•æ£€æŸ¥é€šè¿‡: {dir_path}")
        
        # åˆ›å»ºImageSetsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self.imagesets_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ¸…æ´—åçš„è¾“å‡ºç›®å½•
        self.annotations_output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"åˆ›å»ºæ¸…æ´—è¾“å‡ºç›®å½•: {self.annotations_output_dir}")
        
        logger.info("æ•°æ®é›†åŸºæœ¬ç»“æ„éªŒè¯é€šè¿‡")
    
    def _match_files_parallel(self):
        """å¹¶è¡ŒåŒ¹é…å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶"""
        logger.info("å¼€å§‹å¹¶è¡ŒåŒ¹é…å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶...")
        
        print("ğŸ” æ­£åœ¨æ‰«ææ–‡ä»¶...")
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        for ext in IMAGE_EXTENSIONS:
            image_files.extend(list(self.images_dir.glob(f"*{ext}")))
            image_files.extend(list(self.images_dir.glob(f"*{ext.upper()}")))
        
        # è·å–æ‰€æœ‰XMLæ–‡ä»¶
        xml_files = list(self.annotations_dir.glob(f"*{XML_EXTENSION}"))
        
        print(f"ğŸ“Š å‘ç°å›¾åƒæ–‡ä»¶: {len(image_files)} ä¸ª")
        print(f"ğŸ“Š å‘ç°XMLæ–‡ä»¶: {len(xml_files)} ä¸ª")
        
        # è®°å½•è¯¦ç»†çš„æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ“Š æ–‡ä»¶æ‰«æç»“æœ:")
        logger.info(f"   å›¾åƒæ–‡ä»¶æ€»æ•°: {len(image_files)} ä¸ª")
        logger.info(f"   XMLæ ‡æ³¨æ–‡ä»¶: {len(xml_files)} ä¸ª")
        
        # ç»Ÿè®¡å›¾åƒæ–‡ä»¶ç±»å‹åˆ†å¸ƒ
        image_type_count = {}
        for img in image_files:
            ext = img.suffix.lower()
            image_type_count[ext] = image_type_count.get(ext, 0) + 1
        
        logger.info(f"ğŸ“ˆ å›¾åƒæ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
        for ext, count in sorted(image_type_count.items()):
            logger.info(f"   {ext}: {count} ä¸ª")
        
        # åˆ›å»ºæ–‡ä»¶åæ˜ å°„ï¼ˆä¸å«æ‰©å±•åï¼‰
        print("ğŸ”— åˆ›å»ºæ–‡ä»¶åæ˜ å°„...")
        image_stems = {f.stem: f for f in image_files}
        xml_stems = {f.stem: f for f in xml_files}
        
        # æ£€æŸ¥æ¯ä¸ªXMLæ–‡ä»¶å¯¹åº”çš„å›¾åƒæ˜¯å¦å­˜åœ¨
        print("âœ… éªŒè¯XMLæ–‡ä»¶å¯¹åº”çš„å›¾åƒ...")
        missing_count = 0
        for stem, xml_file in tqdm(xml_stems.items(), desc="æ£€æŸ¥XML->å›¾åƒåŒ¹é…", unit="æ–‡ä»¶"):
            if stem not in image_stems:
                error_msg = f"XMLæ–‡ä»¶æ²¡æœ‰å¯¹åº”çš„å›¾åƒæ–‡ä»¶: {xml_file.absolute()} -> ç¼ºå°‘å›¾åƒ: {stem}"
                logger.error(error_msg)
                self.missing_image_files.append(xml_file)
                missing_count += 1
        
        # å¦‚æœæœ‰XMLæ²¡æœ‰å¯¹åº”å›¾ç‰‡ï¼ŒæŠ¥é”™å¹¶é€€å‡º
        if self.missing_image_files:
            error_msg = f"å‘ç° {len(self.missing_image_files)} ä¸ªXMLæ–‡ä»¶æ²¡æœ‰å¯¹åº”çš„å›¾åƒæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†å®Œæ•´æ€§"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            for xml_file in self.missing_image_files[:5]:
                logger.error(f"  - {xml_file.absolute()}")
            if len(self.missing_image_files) > 5:
                logger.error(f"  ... è¿˜æœ‰ {len(self.missing_image_files) - 5} ä¸ªæ–‡ä»¶")
            raise FileNotFoundError(error_msg)
        
        # æ£€æŸ¥æœ‰å›¾åƒä½†æ²¡æœ‰XMLçš„æ–‡ä»¶ï¼ˆè®°å½•è­¦å‘Šï¼Œä¸æŠ¥é”™ï¼‰
        print("âš ï¸  æ£€æŸ¥ç¼ºå°‘XMLæ ‡æ³¨çš„å›¾åƒ...")
        no_xml_count = 0
        for stem, image_file in tqdm(image_stems.items(), desc="æ£€æŸ¥å›¾åƒ->XMLåŒ¹é…", unit="æ–‡ä»¶"):
            if stem not in xml_stems:
                self.images_without_xml.append(image_file)
                no_xml_count += 1
                if no_xml_count <= 10:
                    logger.warning(f"å›¾åƒæ–‡ä»¶ç¼ºå°‘å¯¹åº”çš„XMLæ ‡æ³¨: {image_file.name}")
        
        if no_xml_count > 10:
            logger.warning(f"è¿˜æœ‰ {no_xml_count - 10} ä¸ªå›¾åƒæ–‡ä»¶ç¼ºå°‘XMLæ ‡æ³¨ï¼ˆå·²çœç•¥æ—¥å¿—ï¼‰")
        
        # å¹¶è¡ŒéªŒè¯æœ‰æ•ˆçš„æ–‡ä»¶å¯¹
        print("ğŸ“ ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒéªŒè¯æ–‡ä»¶å¯¹...")
        valid_stems = set(image_stems.keys()) & set(xml_stems.keys())
        
        logger.info(f"ğŸ§µ å¯åŠ¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {self.max_workers}")
        logger.info(f"ğŸ“Š éœ€è¦éªŒè¯çš„æ–‡ä»¶å¯¹: {len(valid_stems)} ä¸ª")
        
        # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„ä»»åŠ¡
        tasks = []
        for stem in valid_stems:
            image_file = image_stems[stem]
            xml_file = xml_stems[stem]
            tasks.append((image_file, xml_file))
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ–‡ä»¶å¯¹éªŒè¯
        valid_pairs = []
        processed_count = 0
        
        with tqdm(total=len(tasks), desc="ğŸ§µ å¹¶è¡ŒéªŒè¯æ–‡ä»¶å¯¹", unit="å¯¹") as pbar:
            # æäº¤æ‰€æœ‰ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            futures = []
            for image_file, xml_file in tasks:
                future = self.thread_pool.submit(self._validate_file_pair_with_check, image_file, xml_file)
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result:
                        # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                        with self._lock:
                            valid_pairs.append(result)
                            processed_count += 1
                    pbar.update(1)
                except Exception as e:
                    logger.error(f"ğŸš« éªŒè¯æ–‡ä»¶å¯¹æ—¶å‡ºé”™: {e}")
                    pbar.update(1)
        
        self.valid_pairs = valid_pairs
        
        logger.info(f"ğŸ§µ çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†å®Œæˆ:")
        logger.info(f"   å¤„ç†ä»»åŠ¡æ•°: {processed_count}/{len(tasks)}")
        logger.info(f"   æœ‰æ•ˆæ–‡ä»¶å¯¹: {len(valid_pairs)} ä¸ª")
        
        logger.info(f"æ–‡ä»¶åŒ¹é…å®Œæˆ:")
        logger.info(f"  æœ‰æ•ˆæ–‡ä»¶å¯¹: {len(self.valid_pairs)} ä¸ª")
        logger.info(f"  ç¼ºå°‘XMLçš„å›¾åƒ: {len(self.images_without_xml)} ä¸ª")
        logger.info(f"  ç¼ºå°‘å›¾åƒçš„XML: {len(self.missing_image_files)} ä¸ª")
        
        print(f"âœ… å¹¶è¡Œæ–‡ä»¶åŒ¹é…å®Œæˆ: {len(self.valid_pairs)} å¯¹æœ‰æ•ˆæ–‡ä»¶")
        if self.images_without_xml:
            print(f"âš ï¸  å‘ç° {len(self.images_without_xml)} ä¸ªå›¾åƒç¼ºå°‘XMLæ ‡æ³¨ï¼ˆå·²è·³è¿‡ï¼‰")
    
    def _validate_file_pair_with_check(self, image_file: Path, xml_file: Path) -> Tuple[Path, Path]:
        """ä½¿ç”¨çº¿ç¨‹æ± éªŒè¯å•ä¸ªæ–‡ä»¶å¯¹çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ›´è¯¦ç»†çš„æ£€æŸ¥"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not image_file.exists():
                logger.error(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_file.absolute()}")
                return None
            
            if not xml_file.exists():
                logger.error(f"XMLæ–‡ä»¶ä¸å­˜åœ¨: {xml_file.absolute()}")
                return None
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé¿å…ç©ºæ–‡ä»¶ï¼‰
            if image_file.stat().st_size == 0:
                logger.warning(f"å›¾åƒæ–‡ä»¶ä¸ºç©º: {image_file.name}")
                return None
                
            if xml_file.stat().st_size == 0:
                logger.warning(f"XMLæ–‡ä»¶ä¸ºç©º: {xml_file.name}")
                return None
            
            # ç®€å•éªŒè¯XMLæ–‡ä»¶æ ¼å¼
            try:
                import xml.etree.ElementTree as ET
                ET.parse(xml_file)
            except ET.ParseError as e:
                logger.error(f"XMLæ–‡ä»¶æ ¼å¼é”™è¯¯: {xml_file.name} - {e}")
                return None
            
            return (image_file, xml_file)
            
        except Exception as e:
            logger.error(f"éªŒè¯æ–‡ä»¶å¯¹å¤±è´¥: {image_file.name}, {xml_file.name} - {e}")
            return None
    
    def _validate_file_pair(self, image_file: Path, xml_file: Path) -> Tuple[Path, Path]:
        """éªŒè¯å•ä¸ªæ–‡ä»¶å¯¹çš„æœ‰æ•ˆæ€§ï¼ˆä¿ç•™åŸæ–¹æ³•å…¼å®¹æ€§ï¼‰"""
        return self._validate_file_pair_with_check(image_file, xml_file)
    
    def _remove_empty_annotations_and_clean(self):
        """åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶å¹¶æ¸…æ´—XMLåˆ°è¾“å‡ºç›®å½•"""
        logger.info("å¼€å§‹æ£€æŸ¥ç©ºæ ‡æ³¨å¹¶æ¸…æ´—XMLæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•...")
        
        print("ğŸ§¹ æ­£åœ¨æ¸…æ´—XMLæ–‡ä»¶...")
        
        # æ¸…ç©ºè¾“å‡ºç›®å½•
        if self.annotations_output_dir.exists():
            shutil.rmtree(self.annotations_output_dir)
        self.annotations_output_dir.mkdir(parents=True, exist_ok=True)
        
        valid_pairs_after_cleanup = []
        empty_annotations = []
        
        # å¹¶è¡Œå¤„ç†XMLæ–‡ä»¶æ¸…æ´—
        with tqdm(total=len(self.valid_pairs), desc="æ¸…æ´—XMLæ–‡ä»¶", unit="æ–‡ä»¶") as pbar:
            futures = []
            for image_file, xml_file in self.valid_pairs:
                future = self.thread_pool.submit(self._process_xml_file, image_file, xml_file)
                futures.append(future)
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result:
                        if result['is_valid']:
                            valid_pairs_after_cleanup.append((result['image_file'], result['output_xml_file']))
                        else:
                            empty_annotations.append((result['image_file'], result['xml_file']))
                    pbar.update(1)
                except Exception as e:
                    logger.error(f"å¤„ç†XMLæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    pbar.update(1)
        
        # æ›´æ–°æœ‰æ•ˆæ–‡ä»¶å¯¹ï¼Œç°åœ¨æŒ‡å‘æ¸…æ´—åçš„XMLæ–‡ä»¶
        self.valid_pairs = valid_pairs_after_cleanup
        
        # æ›´æ–°annotations_diræŒ‡å‘æ¸…æ´—åçš„ç›®å½•
        self.annotations_dir = self.annotations_output_dir
        
        logger.info(f"XMLæ¸…æ´—å®Œæˆ:")
        logger.info(f"  å‘ç°ç©ºæ ‡æ³¨: {len(empty_annotations)} ä¸ª")
        logger.info(f"  æ¸…æ´—åæœ‰æ•ˆæ–‡ä»¶: {len(self.valid_pairs)} ä¸ª")
        logger.info(f"  æ¸…æ´—è¾“å‡ºç›®å½•: {self.annotations_output_dir}")
        
        print(f"ğŸ§¹ XMLæ¸…æ´—å®Œæˆ:")
        print(f"   å‘ç°ç©ºæ ‡æ³¨: {len(empty_annotations)} ä¸ª")
        print(f"   æ¸…æ´—åæœ‰æ•ˆæ–‡ä»¶: {len(self.valid_pairs)} ä¸ª")
        print(f"   è¾“å‡ºç›®å½•: {self.annotations_output_dir}")
        
        if empty_annotations:
            logger.warning("ä»¥ä¸‹æ–‡ä»¶ä¸ºç©ºæ ‡æ³¨ï¼ˆå·²è·³è¿‡ï¼‰:")
            for i, (img_file, xml_file) in enumerate(empty_annotations[:10]):
                logger.warning(f"  {i+1}. {xml_file.name}")
            if len(empty_annotations) > 10:
                logger.warning(f"  ... è¿˜æœ‰ {len(empty_annotations) - 10} ä¸ªç©ºæ ‡æ³¨æ–‡ä»¶")
    
    def _process_xml_file(self, image_file: Path, xml_file: Path) -> Dict:
        """å¤„ç†å•ä¸ªXMLæ–‡ä»¶ï¼Œä¿æŒåŸæœ‰æ ¼å¼"""
        try:
            # è§£æXMLæ–‡ä»¶
            tree = ET.parse(xml_file)
            root = tree.getroot()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰objectæ ‡ç­¾
            objects = root.findall('object')
            
            if not objects:
                # ç©ºæ ‡æ³¨æ–‡ä»¶ï¼Œä¸å¤åˆ¶åˆ°è¾“å‡ºç›®å½•
                logger.warning(f"å‘ç°ç©ºæ ‡æ³¨æ–‡ä»¶: {xml_file.name}")
                return {
                    'is_valid': False,
                    'image_file': image_file,
                    'xml_file': xml_file,
                    'output_xml_file': None
                }
            else:
                # æœ‰æ•ˆæ ‡æ³¨æ–‡ä»¶ï¼Œå¤åˆ¶åˆ°è¾“å‡ºç›®å½•å¹¶ä¿æŒæ ¼å¼
                output_xml_file = Path(os.path.join(str(self.annotations_output_dir), xml_file.name))
                
                # ä½¿ç”¨minidomä¿æŒåŸæœ‰æ ¼å¼
                self._copy_xml_with_format(xml_file, output_xml_file)
                
                logger.debug(f"æ¸…æ´—XMLæ–‡ä»¶: {xml_file.name} -> {output_xml_file.name}")
                return {
                    'is_valid': True,
                    'image_file': image_file,
                    'xml_file': xml_file,
                    'output_xml_file': output_xml_file
                }
                
        except Exception as e:
            logger.error(f"å¤„ç†XMLæ–‡ä»¶å¤±è´¥: {xml_file.name} - {e}")
            return None
    
    def _copy_xml_with_format(self, source_xml: Path, target_xml: Path):
        """å¤åˆ¶XMLæ–‡ä»¶å¹¶ä¿æŒåŸæœ‰æ ¼å¼ï¼ˆæ¢è¡Œå’Œç¼©è¿›ï¼‰"""
        try:
            # ç›´æ¥å¤åˆ¶æ–‡ä»¶ä»¥ä¿æŒåŸæœ‰æ ¼å¼
            shutil.copy2(source_xml, target_xml)
        except Exception as e:
            logger.error(f"å¤åˆ¶XMLæ–‡ä»¶å¤±è´¥: {source_xml} -> {target_xml} - {e}")
            raise
    
    def _extract_classes(self):
        """æå–æ‰€æœ‰ç±»åˆ«"""
        logger.info("å¼€å§‹æå–ç±»åˆ«ä¿¡æ¯...")
        
        self.classes = set()
        class_count = {}
        
        for image_file, xml_file in tqdm(self.valid_pairs, desc="æå–ç±»åˆ«", unit="æ–‡ä»¶"):
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                # æŸ¥æ‰¾æ‰€æœ‰objectæ ‡ç­¾
                for obj in root.findall('object'):
                    name_elem = obj.find('name')
                    if name_elem is not None and name_elem.text:
                        class_name = name_elem.text.strip()
                        self.classes.add(class_name)
                        class_count[class_name] = class_count.get(class_name, 0) + 1
                        
            except Exception as e:
                logger.error(f"æå–ç±»åˆ«æ—¶è§£æXMLå¤±è´¥: {xml_file.name} - {e}")
        
        logger.info(f"ç±»åˆ«æå–å®Œæˆ:")
        logger.info(f"  å‘ç°ç±»åˆ«: {len(self.classes)} ä¸ª")
        logger.info(f"  ç±»åˆ«åˆ—è¡¨: {sorted(self.classes)}")
        
        # è®°å½•æ¯ä¸ªç±»åˆ«çš„æ•°é‡ç»Ÿè®¡
        logger.info("ğŸ“Š ç±»åˆ«æ•°é‡ç»Ÿè®¡:")
        for class_name in sorted(class_count.keys()):
            logger.info(f"   {class_name}: {class_count[class_name]} ä¸ªå¯¹è±¡")
        
        print(f"ğŸ·ï¸  ç±»åˆ«æå–å®Œæˆ: å‘ç° {len(self.classes)} ä¸ªç±»åˆ«")
    
    def _write_labels_file(self):
        """å†™å…¥æ ‡ç­¾æ–‡ä»¶"""
        logger.info("å†™å…¥æ ‡ç­¾æ–‡ä»¶...")
        
        if not self.classes:
            logger.warning("æ²¡æœ‰ç±»åˆ«ä¿¡æ¯ï¼Œè·³è¿‡æ ‡ç­¾æ–‡ä»¶å†™å…¥")
            return
        
        labels_file = Path(os.path.join(str(self.imagesets_dir), LABELS_TXT))
        
        try:
            with open(labels_file, 'w', encoding=DEFAULT_ENCODING) as f:
                for class_name in sorted(self.classes):
                    f.write(f"{class_name}\n")
            
            logger.info(f"æ ‡ç­¾æ–‡ä»¶å†™å…¥å®Œæˆ: {labels_file}")
            logger.info(f"å†™å…¥ç±»åˆ«: {len(self.classes)} ä¸ª")
            
        except Exception as e:
            logger.error(f"å†™å…¥æ ‡ç­¾æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def check_and_fix_image_dimensions_parallel(self, auto_fix: bool = False):
        """
        å¹¶è¡Œæ£€æŸ¥XMLä¸­è®°å½•çš„å›¾åƒå°ºå¯¸ä¿¡æ¯æ˜¯å¦ä¸å®é™…å›¾åƒåŒ¹é…
        
        Args:
            auto_fix: æ˜¯å¦è‡ªåŠ¨ä¿®æ­£ä¸åŒ¹é…çš„ä¿¡æ¯
        
        Returns:
            dict: æ£€æŸ¥ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹å¹¶è¡Œæ£€æŸ¥å›¾åƒå°ºå¯¸ä¿¡æ¯ - è‡ªåŠ¨ä¿®æ­£: {auto_fix}")
        
        stats = {
            'total_checked': 0,
            'dimension_mismatches': 0,
            'channel_mismatches': 0,
            'read_errors': 0,
            'fixed_xmls': 0,
            'converted_images': 0,
            'mismatch_details': []
        }
        
        # æ¸…ç©ºä¹‹å‰çš„è®°å½•
        self.dimension_mismatches = []
        self.channel_mismatches = []
        
        # å¹¶è¡Œå¤„ç†å›¾åƒå°ºå¯¸æ£€æŸ¥
        print("ğŸ“ å¹¶è¡Œæ£€æŸ¥å›¾åƒå°ºå¯¸...")
        with tqdm(total=len(self.valid_pairs), desc="æ£€æŸ¥å›¾åƒå°ºå¯¸", unit="æ–‡ä»¶") as pbar:
            futures = []
            for image_file, xml_file in self.valid_pairs:
                future = self.thread_pool.submit(self._check_single_image_dimension, image_file, xml_file, auto_fix)
                futures.append(future)
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result:
                        # çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        with self._lock:
                            stats['total_checked'] += 1
                            if result.get('dimension_mismatch'):
                                stats['dimension_mismatches'] += 1
                                self.dimension_mismatches.append(result)
                            if result.get('channel_mismatch'):
                                stats['channel_mismatches'] += 1
                                self.channel_mismatches.append(result)
                            if result.get('read_error'):
                                stats['read_errors'] += 1
                            if result.get('fixed_xml'):
                                stats['fixed_xmls'] += 1
                            if result.get('converted_image'):
                                stats['converted_images'] += 1
                            if result.get('mismatch_details'):
                                stats['mismatch_details'].append(result['mismatch_details'])
                    pbar.update(1)
                except Exception as e:
                    logger.error(f"æ£€æŸ¥å›¾åƒå°ºå¯¸æ—¶å‡ºé”™: {e}")
                    pbar.update(1)
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        logger.info(f"ğŸ“Š å¹¶è¡Œå›¾åƒå°ºå¯¸æ£€æŸ¥å®Œæˆ:")
        logger.info(f"  æ£€æŸ¥æ–‡ä»¶: {stats['total_checked']} ä¸ª")
        logger.info(f"  å°ºå¯¸ä¸åŒ¹é…: {stats['dimension_mismatches']} ä¸ª")
        logger.info(f"  é€šé“æ•°ä¸åŒ¹é…: {stats['channel_mismatches']} ä¸ª")
        logger.info(f"  è¯»å–é”™è¯¯: {stats['read_errors']} ä¸ª")
        
        if auto_fix:
            logger.info(f"  ä¿®æ­£XML: {stats['fixed_xmls']} ä¸ª")
            logger.info(f"  è½¬æ¢å›¾åƒ: {stats['converted_images']} ä¸ª")
        
        print(f"ğŸ“ å¹¶è¡Œå›¾åƒå°ºå¯¸æ£€æŸ¥å®Œæˆ:")
        print(f"   æ£€æŸ¥æ–‡ä»¶: {stats['total_checked']} ä¸ª")
        print(f"   å°ºå¯¸ä¸åŒ¹é…: {stats['dimension_mismatches']} ä¸ª")
        print(f"   é€šé“æ•°ä¸åŒ¹é…: {stats['channel_mismatches']} ä¸ª")
        if auto_fix:
            print(f"   ä¿®æ­£XML: {stats['fixed_xmls']} ä¸ª")
            print(f"   è½¬æ¢å›¾åƒ: {stats['converted_images']} ä¸ª")
        
        return stats
    
    def _check_single_image_dimension(self, image_file: Path, xml_file: Path, auto_fix: bool = False) -> Dict:
        """æ£€æŸ¥å•ä¸ªå›¾åƒçš„å°ºå¯¸ä¿¡æ¯"""
        try:
            # è¯»å–XMLä¸­çš„å°ºå¯¸ä¿¡æ¯
            tree = ET.parse(xml_file)
            root = tree.getroot()
            
            size_elem = root.find('size')
            if size_elem is None:
                logger.warning(f"XMLæ–‡ä»¶ç¼ºå°‘sizeæ ‡ç­¾: {xml_file.name}")
                return None
            
            # è·å–XMLä¸­è®°å½•çš„å°ºå¯¸
            xml_width = size_elem.find('width')
            xml_height = size_elem.find('height')
            xml_depth = size_elem.find('depth')
            
            if xml_width is None or xml_height is None or xml_depth is None:
                logger.warning(f"XMLæ–‡ä»¶sizeæ ‡ç­¾ä¸å®Œæ•´: {xml_file.name}")
                return None
            
            xml_w = int(xml_width.text)
            xml_h = int(xml_height.text)
            xml_d = int(xml_depth.text)
            
            # è¯»å–å®é™…å›¾åƒ
            img = cv2.imread(str(image_file))
            if img is None:
                logger.error(f"æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {image_file.name}")
                return {'read_error': True}
            
            # è·å–å®é™…å›¾åƒå°ºå¯¸
            actual_h, actual_w, actual_d = img.shape
            
            # æ£€æŸ¥å°ºå¯¸æ˜¯å¦åŒ¹é…
            dimension_mismatch = (xml_w != actual_w or xml_h != actual_h)
            channel_mismatch = (xml_d != actual_d or actual_d != 3)
            
            result = {
                'xml_file': str(xml_file.absolute()),
                'image_file': str(image_file.absolute()),
                'xml_size': (xml_w, xml_h, xml_d),
                'actual_size': (actual_w, actual_h, actual_d),
                'dimension_mismatch': dimension_mismatch,
                'channel_mismatch': channel_mismatch,
                'fixed_xml': False,
                'converted_image': False
            }
            
            if dimension_mismatch or channel_mismatch:
                if dimension_mismatch:
                    warning_msg = f"ğŸ“ å°ºå¯¸ä¸åŒ¹é… - {xml_file.name}: XML({xml_w}x{xml_h}) vs å®é™…({actual_w}x{actual_h})"
                    logger.warning(warning_msg)
                
                if channel_mismatch:
                    warning_msg = f"ğŸ¨ é€šé“æ•°ä¸åŒ¹é… - {xml_file.name}: XML({xml_d}) vs å®é™…({actual_d})"
                    logger.warning(warning_msg)
                
                # å¦‚æœå¯ç”¨è‡ªåŠ¨ä¿®æ­£
                if auto_fix:
                    # å¤„ç†å›¾åƒé€šé“æ•°
                    img_modified = False
                    if actual_d != 3:
                        if actual_d == 1:
                            # ç°åº¦å›¾è½¬RGB
                            img_fixed = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
                            img_modified = True
                            logger.info(f"ğŸ”§ è½¬æ¢ç°åº¦å›¾ä¸ºRGB: {image_file.name}")
                        elif actual_d == 4:
                            # RGBAè½¬RGB
                            img_fixed = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)
                            img_modified = True
                            logger.info(f"ğŸ”§ è½¬æ¢RGBAä¸ºRGB: {image_file.name}")
                        else:
                            logger.error(f"ä¸æ”¯æŒçš„é€šé“æ•°: {actual_d} - {image_file.name}")
                            return result
                        
                        if img_modified:
                            # è¦†ç›–åŸå›¾åƒ
                            cv2.imwrite(str(image_file), img_fixed)
                            logger.info(f"âœ… å·²è½¬æ¢å›¾åƒä¸º3é€šé“å¹¶è¦†ç›–åŸå›¾: {image_file.name}")
                            result['converted_image'] = True
                            
                            # æ›´æ–°å®é™…å°ºå¯¸ä¿¡æ¯
                            actual_h, actual_w, actual_d = img_fixed.shape
                    
                    # ä¿®æ­£XMLä¸­çš„å°ºå¯¸ä¿¡æ¯
                    xml_width.text = str(actual_w)
                    xml_height.text = str(actual_h)
                    xml_depth.text = "3"  # å¼ºåˆ¶è®¾ä¸º3é€šé“
                    
                    # ä¿å­˜ä¿®æ­£åçš„XML
                    tree.write(xml_file, encoding=DEFAULT_ENCODING, xml_declaration=True)
                    logger.info(f"âœ… å·²ä¿®æ­£XMLå°ºå¯¸ä¿¡æ¯: {xml_file.name} -> ({actual_w}x{actual_h}x3)")
                    result['fixed_xml'] = True
                
                result['mismatch_details'] = {
                    'xml_file': str(xml_file.absolute()),
                    'image_file': str(image_file.absolute()),
                    'xml_size': (xml_w, xml_h, xml_d),
                    'actual_size': (actual_w, actual_h, actual_d),
                    'dimension_mismatch': dimension_mismatch,
                    'channel_mismatch': channel_mismatch
                }
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†å›¾åƒæ–‡ä»¶å¤±è´¥: {image_file.name} - {e}")
            return {'read_error': True}
    
    def _split_dataset(self):
        """æ•°æ®é›†åˆ’åˆ†åŠŸèƒ½"""
        logger.info("å¼€å§‹æ•°æ®é›†åˆ’åˆ†...")
        print("ğŸ“Š æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
        
        if not self.valid_pairs:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯¹ï¼Œè·³è¿‡æ•°æ®é›†åˆ’åˆ†")
            return {"success": False, "message": "æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯¹"}
        
        # åˆ›å»ºImageSets/Mainç›®å½•
        self.imagesets_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ–‡ä»¶ååˆ°å®Œæ•´è·¯å¾„çš„æ˜ å°„
        print("ğŸ—‚ï¸  åˆ›å»ºæ–‡ä»¶æ˜ å°„...")
        stem_to_pair = {pair[0].stem: pair for pair in self.valid_pairs}
        file_names = list(stem_to_pair.keys())
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        random.seed(RANDOM_SEED)
        random.shuffle(file_names)
        
        total_count = len(file_names)
        train_count = int(total_count * self.train_ratio)
        val_count = int(total_count * self.val_ratio)
        
        print(f"ğŸ“ˆ åˆ’åˆ†ç»Ÿè®¡: æ€»è®¡ {total_count} ä¸ªæ–‡ä»¶")
        print(f"   - è®­ç»ƒé›†: {train_count} ä¸ª ({self.train_ratio*100:.1f}%)")
        print(f"   - éªŒè¯é›†: {val_count} ä¸ª ({self.val_ratio*100:.1f}%)")
        print(f"   - æµ‹è¯•é›†: {total_count - train_count - val_count} ä¸ª ({self.test_ratio*100:.1f}%)")
        
        # åˆ’åˆ†æ•°æ®é›†
        train_files = file_names[:train_count]
        val_files = file_names[train_count:train_count + val_count]
        test_files = file_names[train_count + val_count:]
        
        # åˆ›å»ºtrainvalé›†åˆï¼ˆè®­ç»ƒé›†+éªŒè¯é›†ï¼‰
        trainval_files = train_files + val_files
        
        # å†™å…¥æ–‡ä»¶
        print("ğŸ’¾ å†™å…¥åˆ’åˆ†æ–‡ä»¶...")
        self._write_split_file_optimized(TRAIN_TXT, train_files, stem_to_pair)
        self._write_split_file_optimized(VAL_TXT, val_files, stem_to_pair)
        if test_files:
            self._write_split_file_optimized(TEST_TXT, test_files, stem_to_pair)
        self._write_split_file_optimized(TRAINVAL_TXT, trainval_files, stem_to_pair)
        
        logger.info(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        logger.info(f"  è®­ç»ƒé›†: {len(train_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"  éªŒè¯é›†: {len(val_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"  æµ‹è¯•é›†: {len(test_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"  è®­ç»ƒéªŒè¯é›†: {len(trainval_files)} ä¸ªæ–‡ä»¶")
        
        print("âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ!")
        
        return {
            "success": True,
            "message": "æ•°æ®é›†åˆ’åˆ†å®Œæˆ",
            "train_count": len(train_files),
            "val_count": len(val_files),
            "test_count": len(test_files)
        }
    
    def _write_split_file_optimized(self, filename: str, file_list: List[str], stem_to_pair: dict):
        """å†™å…¥åˆ’åˆ†æ–‡ä»¶ - ä¼˜åŒ–ç‰ˆæœ¬"""
        file_path = Path(os.path.join(str(self.imagesets_dir), filename))
        
        try:
            with open(file_path, 'w', encoding=DEFAULT_ENCODING) as f:
                for file_name in tqdm(file_list, desc=f"å†™å…¥{filename}", unit="æ–‡ä»¶"):
                    if file_name in stem_to_pair:
                        img_file, xml_file = stem_to_pair[file_name]
                        # å†™å…¥æ ¼å¼: å›¾åƒè·¯å¾„\tæ ‡æ³¨è·¯å¾„
                        image_path = os.path.join(JPEGS_DIR, img_file.name)
                        # æ³¨æ„ï¼šç°åœ¨XMLæ–‡ä»¶åœ¨æ¸…æ´—åçš„ç›®å½•ä¸­
                        annotation_path = os.path.join(ANNOTATIONS_OUTPUT_DIR, xml_file.name)
                        line_content = f"{image_path}\t{annotation_path}\n"
                        f.write(line_content)
            
            logger.debug(f"å†™å…¥åˆ’åˆ†æ–‡ä»¶: {filename} ({len(file_list)} ä¸ªæ–‡ä»¶)")
            
        except Exception as e:
            logger.error(f"å†™å…¥åˆ’åˆ†æ–‡ä»¶å¤±è´¥: {filename} - {e}")
    
    def _convert_to_coco(self):
        """è½¬æ¢ä¸ºCOCOæ ¼å¼"""
        logger.info("å¼€å§‹è½¬æ¢ä¸ºCOCOæ ¼å¼...")
        print("ğŸ”„ æ­£åœ¨è½¬æ¢ä¸ºCOCOæ ¼å¼...")
        
        try:
            # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
            labels_file = Path(os.path.join(str(self.imagesets_dir), LABELS_TXT))
            
            if not labels_file.exists():
                error_msg = f"{LABELS_TXT}æ–‡ä»¶ä¸å­˜åœ¨: {labels_file}"
                logger.error(error_msg)
                return {"success": False, "message": error_msg}
            
            # è¯»å–ç±»åˆ«ä¿¡æ¯
            with open(labels_file, 'r', encoding=DEFAULT_ENCODING) as f:
                label_lines = f.readlines()
                categories = [{'id': i + 1, 'name': label.strip()} for i, label in enumerate(label_lines)]
            
            logger.info(f"ğŸ“‹ åŠ è½½äº† {len(categories)} ä¸ªç±»åˆ«")
            
            # åªè½¬æ¢trainå’Œvalé›†
            result_files = {}
            for split in ['train', 'val']:
                list_file = Path(os.path.join(str(self.imagesets_dir), f"{split}.txt"))
                if not list_file.exists():
                    logger.warning(f"{split}.txt ä¸å­˜åœ¨ï¼Œè·³è¿‡ {split} é›†è½¬æ¢")
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å†…å®¹
                with open(list_file, 'r', encoding=DEFAULT_ENCODING) as f:
                    lines = [line.strip() for line in f.readlines() if line.strip()]
                
                if not lines:
                    logger.warning(f"{split}.txt æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡ {split} é›†è½¬æ¢")
                    continue
                
                output_json = Path(os.path.join(str(self.dataset_path), f"{split}_coco.json"))
                self._convert_split_to_coco_optimized(list_file, categories, output_json)
                result_files[split] = str(output_json)
                logger.info(f"âœ… {split} é›†è½¬æ¢å®Œæˆ: {output_json}")
            
            print("âœ… COCOæ ¼å¼è½¬æ¢å®Œæˆ!")
            return {"success": True, "message": "COCOæ ¼å¼è½¬æ¢å®Œæˆ", "files": result_files}
            
        except Exception as e:
            error_msg = f"COCOè½¬æ¢å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return {"success": False, "message": error_msg}
    
    def _convert_split_to_coco_optimized(self, list_file: Path, categories: List[Dict], output_json: Path):
        """è½¬æ¢å•ä¸ªæ•°æ®é›†åˆ’åˆ†åˆ°COCOæ ¼å¼"""
        import json
        from xml.etree import ElementTree as ET
        
        images = []
        annotations = []
        annotation_id = 1
        
        # åˆ›å»ºç±»åˆ«åç§°åˆ°IDçš„æ˜ å°„
        category_name_to_id = {cat['name']: cat['id'] for cat in categories}
        
        # è¯»å–å›¾åƒåˆ—è¡¨
        with open(list_file, 'r', encoding=DEFAULT_ENCODING) as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"ğŸ“ å¤„ç† {len(lines)} ä¸ªæ–‡ä»¶...")
        
        for i, line in enumerate(tqdm(lines, desc=f"è½¬æ¢{list_file.stem}", unit="æ–‡ä»¶")):
            # è§£æå›¾åƒè·¯å¾„å’Œæ ‡æ³¨è·¯å¾„
            parts = line.split('\t')
            if len(parts) != 2:
                logger.warning(f"è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ: {line}")
                continue
                
            image_path, annotation_path = parts
            image_id = i + 1
            image_filename = Path(image_path).name
            
            # ä»XMLæ–‡ä»¶æå–å›¾åƒå°ºå¯¸
            xml_path = Path(os.path.join(str(self.dataset_path), annotation_path))
            if not xml_path.exists():
                logger.warning(f"XMLæ–‡ä»¶ä¸å­˜åœ¨: {xml_path}")
                continue
                
            try:
                tree = ET.parse(xml_path)
                root = tree.getroot()
                
                size_elem = root.find('size')
                if size_elem is None:
                    logger.warning(f"XMLæ–‡ä»¶ä¸­æ²¡æœ‰sizeä¿¡æ¯: {xml_path}")
                    continue
                    
                image_height = int(size_elem.find('height').text)
                image_width = int(size_elem.find('width').text)
                
                # æ·»åŠ å›¾åƒä¿¡æ¯
                images.append({
                    'id': image_id,
                    'file_name': image_filename,
                    'height': image_height,
                    'width': image_width,
                    'license': None,
                    'flickr_url': None,
                    'coco_url': None,
                    'date_captured': None
                })
                
                # è§£ææ ‡æ³¨ä¿¡æ¯
                objects = root.findall('object')
                for obj in objects:
                    # è·å–ç±»åˆ«åç§°
                    name_elem = obj.find('name')
                    if name_elem is None:
                        continue
                    label = name_elem.text
                    
                    # ä½¿ç”¨æ˜ å°„å¿«é€ŸæŸ¥æ‰¾ç±»åˆ«ID
                    category_id = category_name_to_id.get(label)
                    if category_id is None:
                        logger.warning(f"æœªçŸ¥ç±»åˆ« '{label}' åœ¨æ–‡ä»¶ {xml_path}")
                        continue
                    
                    # è·å–è¾¹ç•Œæ¡†
                    bbox_elem = obj.find('bndbox')
                    if bbox_elem is None:
                        continue
                        
                    xmin = int(bbox_elem.find('xmin').text)
                    ymin = int(bbox_elem.find('ymin').text)
                    xmax = int(bbox_elem.find('xmax').text)
                    ymax = int(bbox_elem.find('ymax').text)
                    
                    # è®¡ç®—COCOæ ¼å¼çš„è¾¹ç•Œæ¡† [x, y, width, height]
                    width = xmax - xmin
                    height = ymax - ymin
                    area = width * height
                    
                    # æ·»åŠ æ ‡æ³¨ä¿¡æ¯
                    annotations.append({
                        'id': annotation_id,
                        'image_id': image_id,
                        'category_id': category_id,
                        'bbox': [xmin, ymin, width, height],
                        'area': area,
                        'iscrowd': 0,
                        'segmentation': []
                    })
                    annotation_id += 1
                    
            except ET.ParseError as e:
                logger.error(f"XMLè§£æé”™è¯¯ {xml_path}: {e}")
                continue
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {xml_path}: {e}")
                continue
        
        # æ„å»ºCOCOæ ¼å¼æ•°æ®
        coco_data = {
            'info': {
                'description': f'{self.dataset_name} Dataset',
                'url': '',
                'version': '1.0',
                'year': 2024,
                'contributor': 'VOC to COCO Converter',
                'date_created': '2024-01-01'
            },
            'licenses': [],
            'images': images,
            'annotations': annotations,
            'categories': categories
        }
        
        # å†™å…¥JSONæ–‡ä»¶
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(coco_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"COCOæ ¼å¼è½¬æ¢å®Œæˆ: {len(images)} å¼ å›¾åƒ, {len(annotations)} ä¸ªæ ‡æ³¨")

    def one_click_complete_conversion(self, skip_confirmation=False):
        """ä¸€é”®å®Œæˆè½¬æ¢å¹¶ä¿®å¤æ‰€æœ‰é—®é¢˜"""
        try:
            logger.info("=" * 80)
            logger.info("ğŸš€ å¼€å§‹ä¸€é”®å®Œæˆè½¬æ¢å¤„ç†ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰")
            logger.info("=" * 80)
            logger.info(f"æ•°æ®é›†åç§°: {self.dataset_name}")
            logger.info(f"æ•°æ®é›†è·¯å¾„: {self.dataset_path}")
            logger.info(f"çº¿ç¨‹æ± é…ç½®: {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
            logger.info(f"å¤„ç†æ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            if not skip_confirmation:
                print("\nâš ï¸  æ³¨æ„ï¼šè¯·ç¡®ä¿å·²å¤‡ä»½åŸå§‹æ•°æ®é›†ï¼")
                print("ğŸš€ å¼€å§‹ä¸€é”®å®Œæˆè½¬æ¢...")
                print("âš ï¸  æ³¨æ„ï¼šæ­¤æ“ä½œå°†ä¿®æ”¹æ‚¨çš„æ•°æ®é›†æ–‡ä»¶ï¼")
                
                user_input = input("\nè¯·ç¡®è®¤æ‚¨å·²å¤‡ä»½æ•°æ®é›† (è¾“å…¥ Y ç»§ç»­ï¼ŒN å–æ¶ˆ): ")
                if user_input.lower() != 'y':
                    logger.info("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                    print("âŒ æ“ä½œå·²å–æ¶ˆ")
                    return {"success": False, "message": "ç”¨æˆ·å–æ¶ˆæ“ä½œ"}
                
                logger.info("âœ… ç”¨æˆ·ç¡®è®¤ç»§ç»­å¤„ç†")
            
            print("\nğŸ“‹ æ­¥éª¤1: æ•°æ®éªŒè¯å’Œæ¸…æ´—...")
            logger.info("ğŸ“‹ æ­¥éª¤1: å¼€å§‹æ•°æ®éªŒè¯å’Œæ¸…æ´—")
            
            # éªŒè¯åŸºæœ¬ç»“æ„
            logger.info("ğŸ” éªŒè¯æ•°æ®é›†åŸºæœ¬ç»“æ„...")
            self._validate_basic_structure()
            logger.info("âœ… æ•°æ®é›†ç»“æ„éªŒè¯å®Œæˆ")
            
            # å¹¶è¡ŒåŒ¹é…æ–‡ä»¶
            logger.info("ğŸ”— å¼€å§‹å¹¶è¡ŒåŒ¹é…å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶...")
            self._match_files_parallel()
            logger.info(f"âœ… å¹¶è¡Œæ–‡ä»¶åŒ¹é…å®Œæˆ - æœ‰æ•ˆæ–‡ä»¶å¯¹: {len(self.valid_pairs)} ä¸ª")
            
            # åˆ é™¤ç©ºæ ‡æ³¨å¹¶æ¸…æ´—XML
            logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†ç©ºæ ‡æ³¨æ–‡ä»¶å¹¶æ¸…æ´—XML...")
            empty_count_before = len(self.valid_pairs)
            self._remove_empty_annotations_and_clean()
            empty_count_after = len(self.valid_pairs)
            removed_empty = empty_count_before - empty_count_after
            logger.info(f"âœ… XMLæ¸…æ´—å®Œæˆ - åˆ é™¤ç©ºæ ‡æ³¨: {removed_empty} ä¸ª")
            
            # æå–ç±»åˆ«
            logger.info("ğŸ·ï¸  å¼€å§‹æå–ç±»åˆ«ä¿¡æ¯...")
            self._extract_classes()
            logger.info(f"âœ… ç±»åˆ«æå–å®Œæˆ - å‘ç°ç±»åˆ«: {len(self.classes)} ä¸ª")
            logger.info(f"ğŸ“ ç±»åˆ«åˆ—è¡¨: {sorted(list(self.classes))}")
            
            # å†™å…¥æ ‡ç­¾æ–‡ä»¶
            logger.info("ğŸ’¾ å†™å…¥æ ‡ç­¾æ–‡ä»¶...")
            self._write_labels_file()
            logger.info("âœ… æ ‡ç­¾æ–‡ä»¶å†™å…¥å®Œæˆ")
            
            print("\nğŸ”§ æ­¥éª¤2: å¹¶è¡Œå›¾åƒå°ºå¯¸æ£€æŸ¥å’Œä¿®æ­£...")
            logger.info("ğŸ“‹ æ­¥éª¤2: å¼€å§‹å¹¶è¡Œå›¾åƒå°ºå¯¸æ£€æŸ¥å’Œä¿®æ­£")
            
            # å¹¶è¡Œæ£€æŸ¥å¹¶ä¿®æ­£å›¾åƒå°ºå¯¸
            logger.info("ğŸ“ å¼€å§‹å¹¶è¡Œæ£€æŸ¥å›¾åƒå°ºå¯¸ä¸€è‡´æ€§...")
            dimension_stats = self.check_and_fix_image_dimensions_parallel(auto_fix=True)
            logger.info("âœ… å¹¶è¡Œå›¾åƒå°ºå¯¸æ£€æŸ¥å’Œä¿®æ­£å®Œæˆ")
            logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            logger.info(f"   æ£€æŸ¥æ–‡ä»¶: {dimension_stats.get('total_checked', 0)} ä¸ª")
            logger.info(f"   å°ºå¯¸ä¸åŒ¹é…: {dimension_stats.get('dimension_mismatches', 0)} ä¸ª")
            logger.info(f"   é€šé“æ•°ä¿®æ­£: {dimension_stats.get('converted_images', 0)} ä¸ª")
            logger.info(f"   XMLä¿®æ­£: {dimension_stats.get('fixed_xmls', 0)} ä¸ª")
            
            print("\nğŸ“Š æ­¥éª¤3: æ•°æ®é›†åˆ’åˆ†...")
            logger.info("ğŸ“‹ æ­¥éª¤3: å¼€å§‹æ•°æ®é›†åˆ’åˆ†")
            
            # åˆ’åˆ†æ•°æ®é›†
            split_result = self._split_dataset()
            logger.info("âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ")
            if split_result.get('success'):
                logger.info(f"ğŸ“Š åˆ’åˆ†ç»“æœ:")
                logger.info(f"   è®­ç»ƒé›†: {split_result.get('train_count', 0)} ä¸ª")
                logger.info(f"   éªŒè¯é›†: {split_result.get('val_count', 0)} ä¸ª")
                logger.info(f"   æµ‹è¯•é›†: {split_result.get('test_count', 0)} ä¸ª")
            
            print("\nğŸ”„ æ­¥éª¤4: COCOæ ¼å¼è½¬æ¢...")
            logger.info("ğŸ“‹ æ­¥éª¤4: å¼€å§‹COCOæ ¼å¼è½¬æ¢")
            
            # è½¬æ¢ä¸ºCOCOæ ¼å¼
            coco_result = self._convert_to_coco()
            logger.info("âœ… COCOæ ¼å¼è½¬æ¢å®Œæˆ")
            
            # æœ€ç»ˆç»Ÿè®¡
            final_stats = {
                "success": True,
                "message": "ä¸€é”®è½¬æ¢å®Œæˆ",
                "valid_pairs": len(self.valid_pairs) if hasattr(self, 'valid_pairs') else 0,
                "classes": len(self.classes) if hasattr(self, 'classes') else 0,
                "dimension_mismatches": len(self.dimension_mismatches) if hasattr(self, 'dimension_mismatches') else 0,
                "channel_mismatches": len(self.channel_mismatches) if hasattr(self, 'channel_mismatches') else 0,
                "missing_xml_files": len(self.missing_xml_files) if hasattr(self, 'missing_xml_files') else 0,
                "missing_image_files": len(self.missing_image_files) if hasattr(self, 'missing_image_files') else 0,
                "annotations_output_dir": str(self.annotations_output_dir)
            }
            
            logger.info("=" * 80)
            logger.info("ğŸ‰ ä¸€é”®è½¬æ¢å¤„ç†å®Œæˆï¼")
            logger.info("=" * 80)
            logger.info("ğŸ“Š æœ€ç»ˆå¤„ç†ç»Ÿè®¡:")
            logger.info(f"   æœ‰æ•ˆæ–‡ä»¶å¯¹: {final_stats['valid_pairs']} ä¸ª")
            logger.info(f"   ç±»åˆ«æ•°é‡: {final_stats['classes']} ä¸ª")
            logger.info(f"   å°ºå¯¸ä¸åŒ¹é…: {final_stats['dimension_mismatches']} ä¸ª")
            logger.info(f"   é€šé“ä¸åŒ¹é…: {final_stats['channel_mismatches']} ä¸ª")
            logger.info(f"   ç¼ºå°‘XML: {final_stats['missing_xml_files']} ä¸ª")
            logger.info(f"   ç¼ºå°‘å›¾åƒ: {final_stats['missing_image_files']} ä¸ª")
            logger.info(f"   æ¸…æ´—è¾“å‡ºç›®å½•: {final_stats['annotations_output_dir']}")
            logger.info("=" * 80)
            
            print("âœ… ä¸€é”®è½¬æ¢å®Œæˆï¼")
            print("ğŸ“‹ è¯¦ç»†å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ä¸­")
            print(f"ğŸ—‚ï¸  æ¸…æ´—åçš„XMLæ–‡ä»¶ä¿å­˜åœ¨: {self.annotations_output_dir}")
            
            return final_stats
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            logger.error("=" * 80)
            logger.error("âŒ ä¸€é”®è½¬æ¢å¤„ç†å¤±è´¥ï¼")
            logger.error("=" * 80)
            logger.error(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"é”™è¯¯ä½ç½®: {__import__('traceback').format_exc()}")
            logger.error("=" * 80)
            
            print(f"âŒ ä¸€é”®è½¬æ¢å¤±è´¥ï¼")
            print(f"â— é”™è¯¯ä¿¡æ¯: {error_msg}")
            print("ğŸ“‹ è¯¦ç»†é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ä¸­")
            
            return {"success": False, "message": error_msg}
        
        finally:
            # ç¡®ä¿çº¿ç¨‹æ± æ­£ç¡®å…³é—­
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=True)
                logger.info("ğŸ§µ çº¿ç¨‹æ± å·²å…³é—­")

    def get_dataset_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'dataset_name': self.dataset_name,
            'dataset_path': str(self.dataset_path),
            'annotations_dir': str(self.annotations_dir),
            'annotations_output_dir': str(self.annotations_output_dir),
            'valid_pairs': len(self.valid_pairs),
            'classes': sorted(list(self.classes)),
            'class_count': len(self.classes),
            'missing_xml_files': len(self.missing_xml_files),
            'missing_image_files': len(self.missing_image_files),
            'train_ratio': self.train_ratio,
            'val_ratio': self.val_ratio,
            'test_ratio': self.test_ratio,
            'max_workers': self.max_workers
        }


        logger.info("ğŸš€ å¼€å§‹ä¸€é”®å®Œæˆè½¬æ¢...")

        # æç¤ºç”¨æˆ·ç¡®è®¤
        print("\n" + "="*80)
        print("âš ï¸  è­¦å‘Šï¼šå³å°†å¼€å§‹å¤„ç†æ•°æ®é›†")
        print("="*80)
        print("ğŸ“‹ å¤„ç†å†…å®¹åŒ…æ‹¬ï¼š")
        print("   1. æ¸…æ´—å’Œä¿®æ­£XMLæ ‡æ³¨æ–‡ä»¶")
        print("   2. ä¿®æ­£å›¾ç‰‡å’ŒXMLå°ºå¯¸ä¸åŒ¹é…é—®é¢˜")
        print("   3. åˆ é™¤ç©ºæ ‡æ³¨å’Œæ— æ•ˆæ–‡ä»¶")
        print("   4. åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†")
        print("   5. è½¬æ¢ä¸ºCOCOæ ¼å¼")
        print("\nğŸ”¥ é‡è¦æé†’ï¼šè¯·ç¡®ä¿å·²å¤‡ä»½åŸå§‹æ•°æ®é›†ï¼")
        print("="*80)

        if not skip_confirmation:
            user_input = input("æ˜¯å¦ç»§ç»­å¤„ç†ï¼Ÿè¾“å…¥ Y ç»§ç»­ï¼ŒN å–æ¶ˆ: ").strip()
            if user_input.upper() != 'Y':
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
                logger.info("ç”¨æˆ·å–æ¶ˆäº†ä¸€é”®è½¬æ¢æ“ä½œ")
                return {"success": False, "message": "ç”¨æˆ·å–æ¶ˆæ“ä½œ"}

            print("\nâœ… ç”¨æˆ·ç¡®è®¤ç»§ç»­ï¼Œå¼€å§‹å¤„ç†...")
            logger.info("ç”¨æˆ·ç¡®è®¤å¼€å§‹ä¸€é”®è½¬æ¢")

            try:
                # åˆ›å»ºæ¸…æ´—åçš„è¾“å‡ºç›®å½•
                self.annotations_output_dir.mkdir(exist_ok=True)
                logger.info(f"åˆ›å»ºæ¸…æ´—è¾“å‡ºç›®å½•: {self.annotations_output_dir}")

                # æ­¥éª¤1: æ•°æ®é›†åŸºæœ¬éªŒè¯
                print("\nğŸ“‹ æ­¥éª¤1: æ•°æ®é›†åŸºæœ¬éªŒè¯...")
                logger.info("å¼€å§‹æ•°æ®é›†åŸºæœ¬éªŒè¯")
                self.validate_dataset()

                # æ­¥éª¤2: æ£€æŸ¥å’Œä¿®æ­£å›¾åƒå°ºå¯¸
                print("\nğŸ”§ æ­¥éª¤2: æ£€æŸ¥å’Œä¿®æ­£å›¾åƒå°ºå¯¸...")
                logger.info("å¼€å§‹æ£€æŸ¥å’Œä¿®æ­£å›¾åƒå°ºå¯¸")
                self.check_and_fix_image_dimensions()

                # æ­¥éª¤3: æ¸…æ´—XMLæ–‡ä»¶
                print("\nğŸ§¹ æ­¥éª¤3: æ¸…æ´—XMLæ–‡ä»¶...")
                logger.info("å¼€å§‹æ¸…æ´—XMLæ–‡ä»¶")
                self._clean_xml_files_parallel()

                # æ­¥éª¤4: åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶
                print("\nğŸ—‘ï¸  æ­¥éª¤4: åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶...")
                logger.info("å¼€å§‹åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶")
                self.remove_empty_annotations()

                # æ­¥éª¤5: æå–ç±»åˆ«ä¿¡æ¯
                print("\nğŸ·ï¸  æ­¥éª¤5: æå–ç±»åˆ«ä¿¡æ¯...")
                logger.info("å¼€å§‹æå–ç±»åˆ«ä¿¡æ¯")
                self.extract_classes()

                # æ­¥éª¤6: åˆ’åˆ†æ•°æ®é›†
                print("\nğŸ“Š æ­¥éª¤6: åˆ’åˆ†æ•°æ®é›†...")
                logger.info("å¼€å§‹åˆ’åˆ†æ•°æ®é›†")
                self.split_dataset()

                # æ­¥éª¤7: è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼ˆåªç”Ÿæˆtrainå’Œvalï¼‰
                print("\nğŸ”„ æ­¥éª¤7: è½¬æ¢ä¸ºCOCOæ ¼å¼...")
                logger.info("å¼€å§‹è½¬æ¢ä¸ºCOCOæ ¼å¼")
                self._convert_to_coco_train_val_only()

                # æ”¶é›†å¤„ç†ç»“æœ
                result = {
                    "success": True,
                    "message": "ä¸€é”®è½¬æ¢å®Œæˆ",
                    "dataset_name": self.dataset_name,
                    "valid_pairs": len(self.valid_pairs),
                    "classes": len(self.classes),
                    "dimension_mismatches": len(self.dimension_mismatches),
                    "channel_mismatches": len(self.channel_mismatches),
                    "missing_xml_files": len(self.missing_xml_files),
                    "missing_image_files": len(self.missing_image_files),
                    "annotations_output_dir": str(self.annotations_output_dir)
                }

                print("\n" + "="*80)
                print("ğŸ‰ ä¸€é”®è½¬æ¢å®Œæˆï¼")
                print("="*80)
                print(f"ğŸ“ æ¸…æ´—åçš„XMLæ–‡ä»¶ä¿å­˜åœ¨: {self.annotations_output_dir}")
                print(f"ğŸ“Š æœ‰æ•ˆæ–‡ä»¶å¯¹: {len(self.valid_pairs)} ä¸ª")
                print(f"ğŸ·ï¸  å‘ç°ç±»åˆ«: {len(self.classes)} ä¸ª")
                print(f"ğŸ“ ä¿®æ­£å°ºå¯¸ä¸åŒ¹é…: {len(self.dimension_mismatches)} ä¸ª")
                print(f"ğŸ¨ ä¿®æ­£é€šé“ä¸åŒ¹é…: {len(self.channel_mismatches)} ä¸ª")
                print("="*80)

                logger.info("ä¸€é”®è½¬æ¢æˆåŠŸå®Œæˆ")
                return result

            except Exception as e:
                error_msg = f"ä¸€é”®è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
                logger.error(error_msg)
                print(f"\nâŒ {error_msg}")
                return {"success": False, "message": error_msg}

    def _clean_xml_files_parallel(self):
        """ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ¸…æ´—XMLæ–‡ä»¶å¹¶ä¿å­˜åˆ°Annotations_clearç›®å½•"""
        xml_files = list(self.annotations_dir.glob("*.xml"))
        
        if not xml_files:
            logger.warning("æœªæ‰¾åˆ°XMLæ–‡ä»¶")
            return
        
        logger.info(f"å¼€å§‹å¹¶è¡Œæ¸…æ´— {len(xml_files)} ä¸ªXMLæ–‡ä»¶")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        futures = []
        for xml_file in xml_files:
            future = self.thread_pool.submit(self._clean_single_xml_file, xml_file)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        cleaned_count = 0
        for future in tqdm(as_completed(futures), total=len(futures), desc="æ¸…æ´—XMLæ–‡ä»¶"):
            try:
                if future.result():
                    cleaned_count += 1
            except Exception as e:
                logger.error(f"æ¸…æ´—XMLæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        logger.info(f"XMLæ–‡ä»¶æ¸…æ´—å®Œæˆï¼ŒæˆåŠŸæ¸…æ´— {cleaned_count}/{len(xml_files)} ä¸ªæ–‡ä»¶")
    
    def _clean_single_xml_file(self, xml_file: Path) -> bool:
        """æ¸…æ´—å•ä¸ªXMLæ–‡ä»¶å¹¶ä¿å­˜åˆ°è¾“å‡ºç›®å½•
        
        Args:
            xml_file: XMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ¸…æ´—
        """
        try:
            # è§£æXMLæ–‡ä»¶
            tree = ET.parse(xml_file)
            root = tree.getroot()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…æ´—
            needs_cleaning = False
            
            # ç§»é™¤æ— æ•ˆçš„objectæ ‡ç­¾
            objects_to_remove = []
            for obj in root.findall('object'):
                name_elem = obj.find('name')
                if name_elem is None or not name_elem.text or name_elem.text.strip() == '':
                    objects_to_remove.append(obj)
                    needs_cleaning = True
                    continue
                
                # æ£€æŸ¥è¾¹ç•Œæ¡†
                bndbox = obj.find('bndbox')
                if bndbox is None:
                    objects_to_remove.append(obj)
                    needs_cleaning = True
                    continue
                
                # æ£€æŸ¥è¾¹ç•Œæ¡†åæ ‡
                try:
                    xmin = int(float(bndbox.find('xmin').text))
                    ymin = int(float(bndbox.find('ymin').text))
                    xmax = int(float(bndbox.find('xmax').text))
                    ymax = int(float(bndbox.find('ymax').text))
                    
                    if xmin >= xmax or ymin >= ymax:
                        objects_to_remove.append(obj)
                        needs_cleaning = True
                except (ValueError, AttributeError):
                    objects_to_remove.append(obj)
                    needs_cleaning = True
            
            # ç§»é™¤æ— æ•ˆå¯¹è±¡
            for obj in objects_to_remove:
                root.remove(obj)
            
            # å¦‚æœéœ€è¦æ¸…æ´—æˆ–è€…å¼ºåˆ¶ä¿å­˜æ¸…æ´—ç‰ˆæœ¬ï¼Œåˆ™ä¿å­˜åˆ°è¾“å‡ºç›®å½•
            output_file = Path(os.path.join(str(self.annotations_output_dir), xml_file.name))
            
            # ä½¿ç”¨minidomä¿æŒæ ¼å¼åŒ–
            rough_string = ET.tostring(root, 'unicode')
            reparsed = minidom.parseString(rough_string)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                # ç§»é™¤ç¬¬ä¸€è¡Œçš„XMLå£°æ˜ï¼Œç„¶åæ·»åŠ è‡ªå®šä¹‰çš„
                lines = reparsed.toprettyxml(indent="  ").split('\n')[1:]
                f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
                f.write('\n'.join(line for line in lines if line.strip()))
            
            if needs_cleaning:
                logger.info(f"æ¸…æ´—å¹¶ä¿å­˜XMLæ–‡ä»¶: {xml_file.name}")
            
            return True
            
        except Exception as e:
            logger.error(f"æ¸…æ´—XMLæ–‡ä»¶ {xml_file.name} æ—¶å‡ºé”™: {e}")
            return False
    
    def _convert_to_coco_train_val_only(self):
        """è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼Œåªç”Ÿæˆtrain_coco.jsonå’Œval_coco.json"""
        try:
            # è¯»å–train.txtå’Œval.txt
            train_file = Path(os.path.join(str(self.imagesets_dir), "train.txt"))
            val_file = Path(os.path.join(str(self.imagesets_dir), "val.txt"))
            
            if train_file.exists():
                logger.info("è½¬æ¢è®­ç»ƒé›†ä¸ºCOCOæ ¼å¼")
                self._convert_split_to_coco("train", train_file)
            
            if val_file.exists():
                logger.info("è½¬æ¢éªŒè¯é›†ä¸ºCOCOæ ¼å¼")
                self._convert_split_to_coco("val", val_file)
            
            # ç¡®ä¿ä¸ç”Ÿæˆtest_coco.json
            test_coco_file = Path(os.path.join(str(self.dataset_path), "test_coco.json"))
            if test_coco_file.exists():
                test_coco_file.unlink()
                logger.info("åˆ é™¤äº†ä¸éœ€è¦çš„test_coco.jsonæ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"è½¬æ¢COCOæ ¼å¼æ—¶å‡ºé”™: {e}")
            raise
    
    def _convert_split_to_coco(self, split_name: str, split_file: Path):
        """è½¬æ¢æŒ‡å®šåˆ’åˆ†ä¸ºCOCOæ ¼å¼
        
        Args:
            split_name: åˆ’åˆ†åç§° (train/val)
            split_file: åˆ’åˆ†æ–‡ä»¶è·¯å¾„
        """
        import json
        from datetime import datetime
        
        # è¯»å–æ–‡ä»¶åˆ—è¡¨
        with open(split_file, 'r', encoding='utf-8') as f:
            file_names = [line.strip() for line in f.readlines()]
        
        # åˆå§‹åŒ–COCOæ ¼å¼æ•°æ®
        coco_data = {
            "info": {
                "description": f"{self.dataset_name} {split_name} dataset",
                "version": "1.0",
                "year": datetime.now().year,
                "contributor": "VOCDataset Converter",
                "date_created": datetime.now().isoformat()
            },
            "licenses": [
                {
                    "id": 1,
                    "name": "Unknown",
                    "url": ""
                }
            ],
            "images": [],
            "annotations": [],
            "categories": []
        }
        
        # æ·»åŠ ç±»åˆ«ä¿¡æ¯
        class_list = sorted(list(self.classes))
        for idx, class_name in enumerate(class_list, 1):
            coco_data["categories"].append({
                "id": idx,
                "name": class_name,
                "supercategory": "object"
            })
        
        # åˆ›å»ºç±»åˆ«åç§°åˆ°IDçš„æ˜ å°„
        class_to_id = {class_name: idx for idx, class_name in enumerate(class_list, 1)}
        
        image_id = 1
        annotation_id = 1
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_name in tqdm(file_names, desc=f"è½¬æ¢{split_name}é›†"):
            # å›¾åƒæ–‡ä»¶è·¯å¾„
            img_file = Path(os.path.join(str(self.images_dir), f"{file_name}.jpg"))
            xml_file = Path(os.path.join(str(self.annotations_output_dir), f"{file_name}.xml"))  # ä½¿ç”¨æ¸…æ´—åçš„XML
            
            if not img_file.exists() or not xml_file.exists():
                continue
            
            # è¯»å–å›¾åƒä¿¡æ¯
            try:
                import cv2
                img = cv2.imread(str(img_file))
                height, width = img.shape[:2]
            except:
                continue
            
            # æ·»åŠ å›¾åƒä¿¡æ¯
            coco_data["images"].append({
                "id": image_id,
                "width": width,
                "height": height,
                "file_name": f"{file_name}.jpg",
                "license": 1,
                "flickr_url": "",
                "coco_url": "",
                "date_captured": ""
            })
            
            # è§£æXMLæ ‡æ³¨
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                for obj in root.findall('object'):
                    name = obj.find('name').text
                    if name not in class_to_id:
                        continue
                    
                    bndbox = obj.find('bndbox')
                    xmin = int(float(bndbox.find('xmin').text))
                    ymin = int(float(bndbox.find('ymin').text))
                    xmax = int(float(bndbox.find('xmax').text))
                    ymax = int(float(bndbox.find('ymax').text))
                    
                    # è®¡ç®—COCOæ ¼å¼çš„è¾¹ç•Œæ¡† [x, y, width, height]
                    bbox_width = xmax - xmin
                    bbox_height = ymax - ymin
                    area = bbox_width * bbox_height
                    
                    # æ·»åŠ æ ‡æ³¨ä¿¡æ¯
                    coco_data["annotations"].append({
                        "id": annotation_id,
                        "image_id": image_id,
                        "category_id": class_to_id[name],
                        "segmentation": [],
                        "area": area,
                        "bbox": [xmin, ymin, bbox_width, bbox_height],
                        "iscrowd": 0
                    })
                    
                    annotation_id += 1
                    
            except Exception as e:
                logger.warning(f"è§£æXMLæ–‡ä»¶ {xml_file.name} æ—¶å‡ºé”™: {e}")
                continue
            
            image_id += 1
        
        # ä¿å­˜COCOæ ¼å¼æ–‡ä»¶
        output_file = Path(os.path.join(str(self.dataset_path), f"{split_name}_coco.json"))
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(coco_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"COCOæ ¼å¼æ–‡ä»¶å·²ä¿å­˜: {output_file}")
        logger.info(f"{split_name}é›†åŒ…å« {len(coco_data['images'])} å¼ å›¾åƒï¼Œ{len(coco_data['annotations'])} ä¸ªæ ‡æ³¨")


if __name__ == "__main__":
    # æµ‹è¯•VOCæ•°æ®é›†ç±»
    dataset_path = os.path.join(".", "dataset", "Fruit")
    
    try:
        voc_dataset = VOCDataset(dataset_path)
        print("æ•°æ®é›†ä¿¡æ¯:", voc_dataset.get_dataset_info())
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯: {e}")

    