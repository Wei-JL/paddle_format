"""
VOCæ•°æ®é›†å¤„ç†ç±»

ç”¨äºå¤„ç†Pascal VOCæ ¼å¼çš„æ•°æ®é›†ï¼Œæä¾›ï¼š
- æ•°æ®é›†åŸºæœ¬éªŒè¯
- å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶åŒ¹é…
- ç©ºæ ‡æ³¨æ–‡ä»¶æ£€æµ‹å’Œåˆ é™¤
- æ•°æ®é›†åˆ’åˆ†åŠŸèƒ½
- ç±»åˆ«æå–åŠŸèƒ½
- ç±»åˆ«è¿‡æ»¤åŠŸèƒ½
- å›¾åƒå°ºå¯¸æ£€æŸ¥å’Œä¿®æ­£åŠŸèƒ½
"""

import os
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Tuple, Set, Dict
import sys
import random
import shutil
import cv2
from tqdm import tqdm
from tqdm import tqdm
import numpy as np

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ - ä½¿ç”¨å…¨å±€å˜é‡
sys.path.append(str(Path(__file__).parent.parent))
from logger_code.logger_sys import get_logger
from global_var.global_cls import *

# è·å–å½“å‰æ–‡ä»¶åä½œä¸ºæ—¥å¿—æ ‡è¯†
current_filename = Path(__file__).stem
logger = get_logger(current_filename)

class VOCDataset:
    """VOCæ•°æ®é›†å¤„ç†ç±»"""
    
    def __init__(self, dataset_path: str, user_labels_file: str = None, 
                 train_ratio: float = TRAIN_RATIO, 
                 val_ratio: float = VAL_RATIO, test_ratio: float = TEST_RATIO):
        """
        åˆå§‹åŒ–VOCæ•°æ®é›†
        
        Args:
            dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
            user_labels_file: ç”¨æˆ·æä¾›çš„æ­£ç¡®æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        """
        self.dataset_path = Path(dataset_path)
        self.dataset_name = self.dataset_path.name
        
        # æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        
        # æ ‡å‡†VOCç›®å½•ç»“æ„
        # æ ‡å‡†VOCç›®å½•ç»“æ„
        self.annotations_dir = Path(os.path.join(str(self.dataset_path), ANNOTATIONS_DIR))
        self.images_dir = Path(os.path.join(str(self.dataset_path), JPEGS_DIR))
        self.imagesets_dir = Path(os.path.join(str(self.dataset_path), IMAGESETS_DIR, MAIN_DIR))
        
        # ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶
        self.user_labels_file = user_labels_file
        self.user_labels = set()  # ç”¨æˆ·æä¾›çš„æ­£ç¡®æ ‡ç­¾é›†åˆ
        
        # è®°å½•ç¼ºå¤±æ–‡ä»¶çš„åˆ—è¡¨
        self.missing_xml_files = []  # ç¼ºå°‘XMLæ–‡ä»¶çš„å›¾åƒ
        self.missing_image_files = []  # ç¼ºå°‘å›¾åƒæ–‡ä»¶çš„XML
        self.images_without_xml = []  # æœ‰å›¾åƒä½†æ²¡æœ‰XMLçš„æ–‡ä»¶åˆ—è¡¨
        
        # æœ‰æ•ˆçš„æ–‡ä»¶å¯¹åˆ—è¡¨
        self.valid_pairs = []
        
        # ç±»åˆ«é›†åˆ
        self.classes = set()
        
        # å°ºå¯¸ä¸åŒ¹é…è®°å½•
        self.dimension_mismatches = []
        self.channel_mismatches = []
        
        logger.info(f"åˆå§‹åŒ–VOCæ•°æ®é›†: {self.dataset_name}")
        logger.info(f"åˆå§‹åŒ–VOCæ•°æ®é›†: {self.dataset_name}")
        logger.info(f"æ•°æ®é›†è·¯å¾„: {self.dataset_path.absolute()}")
        logger.info(f"åˆ’åˆ†æ¯”ä¾‹ - è®­ç»ƒé›†: {self.train_ratio}, éªŒè¯é›†: {self.val_ratio}, æµ‹è¯•é›†: {self.test_ratio}")
        
        # éªŒè¯ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶
        if self.user_labels_file:
            self._validate_user_labels()
        
        # éªŒè¯æ•°æ®é›†åŸºæœ¬ç»“æ„
        self._validate_basic_structure()
        
        # åŒ¹é…å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶
        self._match_files()
        
        # åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶
        self._remove_empty_annotations()
        
        # æå–ç±»åˆ«ä¿¡æ¯
        self._extract_classes()
        
        # éªŒè¯ç±»åˆ«ä¸€è‡´æ€§
        if self.user_labels_file:
            self._validate_class_consistency()
        
        # æ•°æ®é›†åˆ’åˆ†
        self._split_dataset()
    
    def _validate_user_labels(self):
        """éªŒè¯ç”¨æˆ·æä¾›çš„æ ‡ç­¾æ–‡ä»¶"""
        logger.info("éªŒè¯ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶...")
        
        if not self.user_labels_file:
            return
        
        user_labels_path = Path(self.user_labels_file)
        if not user_labels_path.exists():
            # å°è¯•ç›¸å¯¹äºæ•°æ®é›†è·¯å¾„æŸ¥æ‰¾
            user_labels_path = self.dataset_path / self.user_labels_file
            if not user_labels_path.exists():
                error_msg = f"ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {self.user_labels_file}"
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)
        
        # è¯»å–æ ‡ç­¾æ–‡ä»¶å¹¶æ£€æŸ¥é‡å¤
        labels_list = []
        try:
            with open(user_labels_path, 'r', encoding=DEFAULT_ENCODING) as f:
                for line_num, line in enumerate(f, 1):
                    label = line.strip()
                    if label:  # è·³è¿‡ç©ºè¡Œ
                        labels_list.append((line_num, label))
        except Exception as e:
            error_msg = f"è¯»å–ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶å¤±è´¥: {e}"
            logger.error(error_msg)
            raise Exception(error_msg)
        
        # æ£€æŸ¥é‡å¤æ ‡ç­¾
        seen_labels = {}
        for line_num, label in labels_list:
            if label in seen_labels:
                error_msg = f"å‘ç°é‡å¤æ ‡ç­¾ '{label}': ç¬¬{seen_labels[label]}è¡Œä¸ç¬¬{line_num}è¡Œé‡å¤"
                logger.error(error_msg)
                raise ValueError(error_msg)
            seen_labels[label] = line_num
            self.user_labels.add(label)
        
        logger.info(f"ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶éªŒè¯é€šè¿‡ - å…± {len(self.user_labels)} ä¸ªæ ‡ç­¾: {sorted(self.user_labels)}")
        
        # éªŒè¯æ•°æ®é›†åŸºæœ¬ç»“æ„
        self._validate_basic_structure()
        
        # åŒ¹é…å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶
        self._match_files()
        
        # åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶
        self._remove_empty_annotations()
        
        # æå–ç±»åˆ«ä¿¡æ¯
        self._extract_classes()
        
        # æ•°æ®é›†åˆ’åˆ†
        self._split_dataset()
    
    def _validate_basic_structure(self):
        """éªŒè¯æ•°æ®é›†åŸºæœ¬ç»“æ„"""
        logger.info("éªŒè¯æ•°æ®é›†åŸºæœ¬ç»“æ„...")
        
        # æ£€æŸ¥å¿…è¦ç›®å½•æ˜¯å¦å­˜åœ¨
        if not self.annotations_dir.exists():
            error_msg = f"æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {self.annotations_dir}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        if not self.images_dir.exists():
            error_msg = f"å›¾åƒç›®å½•ä¸å­˜åœ¨: {self.images_dir}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        logger.debug(f"ç›®å½•æ£€æŸ¥é€šè¿‡: {ANNOTATIONS_DIR}, {JPEGS_DIR}")
        
        # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€å¼ å›¾ç‰‡
        image_files = []
        for ext in IMAGE_EXTENSIONS:
            image_files.extend(list(self.images_dir.glob(f"*{ext}")))
            image_files.extend(list(self.images_dir.glob(f"*{ext.upper()}")))
        
        if not image_files:
            error_msg = f"å›¾åƒç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {self.images_dir}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªXMLæ–‡ä»¶
        xml_files = list(self.annotations_dir.glob(f"*{XML_EXTENSION}"))
        if not xml_files:
            error_msg = f"æ ‡æ³¨ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°XMLæ–‡ä»¶: {self.annotations_dir}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        logger.info(f"åŸºæœ¬ç»“æ„éªŒè¯é€šè¿‡ - å›¾åƒæ–‡ä»¶: {len(image_files)} ä¸ª, XMLæ–‡ä»¶: {len(xml_files)} ä¸ª")
    
    def _match_files(self):
        """åŒ¹é…å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶ï¼Œå¢å¼ºéªŒè¯"""
        logger.info("å¼€å§‹åŒ¹é…å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶...")
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        for ext in IMAGE_EXTENSIONS:
            image_files.extend(list(self.images_dir.glob(f"*{ext}")))
            image_files.extend(list(self.images_dir.glob(f"*{ext.upper()}")))
        
        # è·å–æ‰€æœ‰XMLæ–‡ä»¶
        xml_files = list(self.annotations_dir.glob(f"*{XML_EXTENSION}"))
        
        # åˆ›å»ºæ–‡ä»¶åæ˜ å°„ï¼ˆä¸å«æ‰©å±•åï¼‰
        image_stems = {f.stem: f for f in image_files}
        xml_stems = {f.stem: f for f in xml_files}
        
        # æ£€æŸ¥æ¯ä¸ªXMLæ–‡ä»¶å¯¹åº”çš„å›¾åƒæ˜¯å¦å­˜åœ¨
        for stem, xml_file in xml_stems.items():
            if stem not in image_stems:
                # XMLæ–‡ä»¶æ²¡æœ‰å¯¹åº”çš„å›¾åƒæ–‡ä»¶ï¼Œè¿™æ˜¯é”™è¯¯
                error_msg = f"XMLæ–‡ä»¶æ²¡æœ‰å¯¹åº”çš„å›¾åƒæ–‡ä»¶: {xml_file.absolute()} -> ç¼ºå°‘å›¾åƒ: {stem}"
                logger.error(error_msg)
                self.missing_image_files.append(xml_file)
        
        # å¦‚æœæœ‰XMLæ²¡æœ‰å¯¹åº”å›¾ç‰‡ï¼ŒæŠ¥é”™å¹¶é€€å‡º
        if self.missing_image_files:
            error_msg = f"å‘ç° {len(self.missing_image_files)} ä¸ªXMLæ–‡ä»¶æ²¡æœ‰å¯¹åº”çš„å›¾åƒæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†å®Œæ•´æ€§"
            logger.error(error_msg)
            for xml_file in self.missing_image_files:
                logger.error(f"  - {xml_file.absolute()}")
            raise FileNotFoundError(error_msg)
        
        # æ£€æŸ¥æœ‰å›¾åƒä½†æ²¡æœ‰XMLçš„æ–‡ä»¶ï¼ˆè®°å½•è­¦å‘Šï¼Œä¸æŠ¥é”™ï¼‰
        for stem, image_file in image_stems.items():
            if stem not in xml_stems:
                self.images_without_xml.append(image_file)
                logger.warning(f"å›¾åƒæ–‡ä»¶ç¼ºå°‘å¯¹åº”çš„XMLæ ‡æ³¨: {image_file.name}")
        
        # æ”¶é›†æœ‰æ•ˆçš„æ–‡ä»¶å¯¹
        for stem in image_stems:
            if stem in xml_stems:
                # éªŒè¯å›¾åƒæ–‡ä»¶æ˜¯å¦çœŸå®å­˜åœ¨ä¸”å¯è¯»
                image_file = image_stems[stem]
                xml_file = xml_stems[stem]
                
                if not image_file.exists():
                    error_msg = f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_file.absolute()}"
                    logger.error(error_msg)
                    raise FileNotFoundError(error_msg)
                
                # å°è¯•è¯»å–å›¾åƒéªŒè¯å…¶æœ‰æ•ˆæ€§
                try:
                    img = cv2.imread(str(image_file))
                    if img is None:
                        error_msg = f"æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {image_file.absolute()}"
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                except Exception as e:
                    error_msg = f"å›¾åƒæ–‡ä»¶è¯»å–å¤±è´¥: {image_file.absolute()} - {e}"
                    logger.error(error_msg)
                    raise ValueError(error_msg)
                
                self.valid_pairs.append((image_file, xml_file))
        
        # ç»Ÿè®¡åŒ¹é…ç»“æœ
        matched_count = len(self.valid_pairs)
        logger.info(f"æ–‡ä»¶åŒ¹é…å®Œæˆ - åŒ¹é…å¯¹æ•°: {matched_count}, ç¼ºå°‘XML: {len(self.images_without_xml)}")
        
        # æœ€åæ‰“å°è­¦å‘Šä¿¡æ¯
        if self.images_without_xml:
            logger.warning(f"å‘ç° {len(self.images_without_xml)} ä¸ªå›¾åƒæ–‡ä»¶æ²¡æœ‰å¯¹åº”çš„XMLæ ‡æ³¨:")
            for img_file in self.images_without_xml:
                logger.warning(f"  - {img_file.name}")
    
    def _remove_empty_annotations(self):
        """åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶"""
        logger.info("æ£€æŸ¥å¹¶åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶...")
        
        empty_files = []
        valid_pairs_after_cleanup = []
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºæ£€æŸ¥è¿›åº¦
        with tqdm(total=len(self.valid_pairs), desc="æ£€æŸ¥å›¾åƒå°ºå¯¸", unit="æ–‡ä»¶", leave=False) as pbar:
            for image_file, xml_file in self.valid_pairs:
                try:
                    tree = ET.parse(xml_file)
                    root = tree.getroot()
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰objectæ ‡ç­¾
                    objects = root.findall('object')
                    if not objects:
                        empty_files.append(xml_file)
                        logger.warning(f"å‘ç°ç©ºæ ‡æ³¨æ–‡ä»¶: {xml_file.name}")
                        
                        # åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶
                        xml_file.unlink()
                        logger.info(f"å·²åˆ é™¤ç©ºæ ‡æ³¨æ–‡ä»¶: {xml_file.name}")
                    else:
                        # ä¿ç•™æœ‰æ•ˆçš„æ–‡ä»¶å¯¹
                        valid_pairs_after_cleanup.append((image_file, xml_file))
                        
                except Exception as e:
                    logger.error(f"è§£æXMLæ–‡ä»¶å¤±è´¥: {xml_file.name} - {e}")
                
                pbar.update(1)
        
        # æ›´æ–°æœ‰æ•ˆæ–‡ä»¶å¯¹åˆ—è¡¨
        self.valid_pairs = valid_pairs_after_cleanup
        
        if empty_files:
            logger.info(f"å…±åˆ é™¤ {len(empty_files)} ä¸ªç©ºæ ‡æ³¨æ–‡ä»¶")
        else:
            logger.info("æœªå‘ç°ç©ºæ ‡æ³¨æ–‡ä»¶")
    
    def _extract_classes(self):
        """æå–æ‰€æœ‰ç±»åˆ«ä¿¡æ¯"""
        logger.info("å¼€å§‹æå–ç±»åˆ«ä¿¡æ¯...")
        
        self.classes = set()
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºç±»åˆ«æå–è¿›åº¦
        with tqdm(total=len(self.valid_pairs), desc="æå–ç±»åˆ«ä¿¡æ¯", unit="æ–‡ä»¶", leave=False) as pbar:
            for _, xml_file in self.valid_pairs:
                try:
                    tree = ET.parse(xml_file)
                    root = tree.getroot()
                    
                    # æŸ¥æ‰¾æ‰€æœ‰objectæ ‡ç­¾ä¸­çš„name
                    for obj in root.findall('object'):
                        name_elem = obj.find('name')
                        if name_elem is not None and name_elem.text:
                            class_name = name_elem.text.strip()
                            self.classes.add(class_name)
                            
                except Exception as e:
                    logger.error(f"è§£æXMLæ–‡ä»¶å¤±è´¥: {xml_file.name} - {e}")
                
                pbar.update(1)
        
        logger.info(f"ç±»åˆ«æå–å®Œæˆ - å‘ç° {len(self.classes)} ä¸ªç±»åˆ«: {sorted(self.classes)}")
        
        # å†™å…¥labels.txtæ–‡ä»¶
        self._write_labels_file()
    
    def _write_labels_file(self):
        """å†™å…¥ç±»åˆ«æ ‡ç­¾æ–‡ä»¶"""
        logger.info("å†™å…¥ç±»åˆ«æ ‡ç­¾æ–‡ä»¶...")
        
        # åˆ›å»ºImageSets/Mainç›®å½•
        self.imagesets_dir.mkdir(parents=True, exist_ok=True)
        
        # å¯¹æ ‡ç­¾è¿›è¡Œæ’åºï¼ˆæ•°å­—ä¼˜å…ˆï¼Œç„¶åå­—æ¯ï¼‰
        sorted_labels = sorted(self.classes, key=lambda x: (x.isdigit(), x))
        
        labels_file_path = self.imagesets_dir / "labels.txt"
        
        try:
            with open(labels_file_path, 'w', encoding=DEFAULT_ENCODING) as f:
                for i, label in enumerate(sorted_labels):
                    if i == len(sorted_labels) - 1:
                        # æœ€åä¸€è¡Œä¸æ¢è¡Œ
                        f.write(label)
                    else:
                        f.write(f"{label}\n")
            
            logger.info(f"ç±»åˆ«æ ‡ç­¾æ–‡ä»¶å†™å…¥å®Œæˆ: {labels_file_path}")
            logger.debug(f"å†™å…¥ {len(sorted_labels)} ä¸ªç±»åˆ«")
            
        except Exception as e:
            logger.error(f"å†™å…¥ç±»åˆ«æ ‡ç­¾æ–‡ä»¶å¤±è´¥: {e}")
    
    def _validate_class_consistency(self):
        """éªŒè¯ç±»åˆ«ä¸€è‡´æ€§"""
        logger.info("éªŒè¯ç±»åˆ«ä¸€è‡´æ€§...")
        
        if not self.user_labels:
            logger.warning("æœªæä¾›ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶ï¼Œè·³è¿‡ç±»åˆ«ä¸€è‡´æ€§éªŒè¯")
            return
        
        # æ£€æŸ¥XMLä¸­çš„ç±»åˆ«æ˜¯å¦éƒ½åœ¨ç”¨æˆ·æ ‡ç­¾ä¸­
        xml_only_classes = self.classes - self.user_labels
        user_only_classes = self.user_labels - self.classes
        
        if xml_only_classes:
            error_msg = f"XMLæ–‡ä»¶ä¸­å‘ç°ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶ä¸­ä¸å­˜åœ¨çš„ç±»åˆ«: {sorted(xml_only_classes)}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        if user_only_classes:
            logger.warning(f"ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶ä¸­æœ‰æœªåœ¨XMLä¸­ä½¿ç”¨çš„ç±»åˆ«: {sorted(user_only_classes)}")
        
        logger.info("ç±»åˆ«ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
    
    def extract_classes_only(self):
        """
        å•ç‹¬æå–ç±»åˆ«ä¿¡æ¯çš„å…¬å…±æ–¹æ³•
        å¯ä»¥åœ¨ä¸è¿›è¡Œæ•°æ®é›†åˆ’åˆ†çš„æƒ…å†µä¸‹å•ç‹¬è°ƒç”¨
        """
        logger.info("å•ç‹¬æå–ç±»åˆ«ä¿¡æ¯...")
        
        if not self.valid_pairs:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯¹ï¼Œæ— æ³•æå–ç±»åˆ«")
            return set()
        
        self._extract_classes()
        return self.classes.copy()
    
    def filter_classes_and_regenerate(self, exclude_classes: List[str], new_annotations_suffix: str = "filtered"):
        """
        è¿‡æ»¤æŒ‡å®šç±»åˆ«å¹¶é‡æ–°ç”Ÿæˆæ•°æ®é›†
        
        Args:
            exclude_classes: è¦å‰”é™¤çš„ç±»åˆ«åˆ—è¡¨
            new_annotations_suffix: æ–°æ ‡æ³¨ç›®å½•çš„åç¼€å
        
        Returns:
            dict: è¿‡æ»¤ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹ç±»åˆ«è¿‡æ»¤ - å‰”é™¤ç±»åˆ«: {exclude_classes}")
        logger.info(f"æ–°æ ‡æ³¨ç›®å½•åç¼€: {new_annotations_suffix}")
        
        # åˆ›å»ºæ–°çš„æ ‡æ³¨ç›®å½•
        new_annotations_dir = self.dataset_path / f"{ANNOTATIONS_DIR}_{new_annotations_suffix}"
        new_annotations_dir.mkdir(exist_ok=True)
        logger.info(f"åˆ›å»ºæ–°æ ‡æ³¨ç›®å½•: {new_annotations_dir}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_files': len(self.valid_pairs),
            'filtered_files': 0,
            'empty_after_filter': 0,
            'valid_after_filter': 0,
            'excluded_classes': exclude_classes,
            'remaining_classes': set()
        }
        
        filtered_pairs = []
        
        # å¤„ç†æ¯ä¸ªXMLæ–‡ä»¶
        for image_file, xml_file in self.valid_pairs:
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                # æŸ¥æ‰¾æ‰€æœ‰objectæ ‡ç­¾
                objects = root.findall('object')
                remaining_objects = []
                
                # è¿‡æ»¤object
                for obj in objects:
                    name_elem = obj.find('name')
                    if name_elem is not None and name_elem.text:
                        class_name = name_elem.text.strip()
                        
                        # å¦‚æœä¸åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼Œä¿ç•™è¯¥object
                        if class_name not in exclude_classes:
                            remaining_objects.append(obj)
                            stats['remaining_classes'].add(class_name)
                
                # ç§»é™¤æ‰€æœ‰åŸæœ‰çš„objectæ ‡ç­¾
                for obj in objects:
                    root.remove(obj)
                
                # æ·»åŠ è¿‡æ»¤åçš„objectæ ‡ç­¾
                for obj in remaining_objects:
                    root.append(obj)
                
                # å¦‚æœè¿˜æœ‰æœ‰æ•ˆçš„objectï¼Œä¿å­˜æ–‡ä»¶
                if remaining_objects:
                    new_xml_path = new_annotations_dir / xml_file.name
                    tree.write(new_xml_path, encoding=DEFAULT_ENCODING, xml_declaration=True)
                    filtered_pairs.append((image_file, new_xml_path))
                    stats['valid_after_filter'] += 1
                    logger.debug(f"è¿‡æ»¤åä¿ç•™æ–‡ä»¶: {xml_file.name} (å‰©ä½™ {len(remaining_objects)} ä¸ªå¯¹è±¡)")
                else:
                    stats['empty_after_filter'] += 1
                    logger.debug(f"è¿‡æ»¤åä¸ºç©ºï¼Œè·³è¿‡æ–‡ä»¶: {xml_file.name}")
                
                stats['filtered_files'] += 1
                
            except Exception as e:
                logger.error(f"å¤„ç†XMLæ–‡ä»¶å¤±è´¥: {xml_file.name} - {e}")
        
        logger.info(f"ç±»åˆ«è¿‡æ»¤å®Œæˆ:")
        logger.info(f"  å¤„ç†æ–‡ä»¶: {stats['filtered_files']} ä¸ª")
        logger.info(f"  æœ‰æ•ˆæ–‡ä»¶: {stats['valid_after_filter']} ä¸ª")
        logger.info(f"  ç©ºæ–‡ä»¶: {stats['empty_after_filter']} ä¸ª")
        logger.info(f"  å‰©ä½™ç±»åˆ«: {sorted(stats['remaining_classes'])}")
        
        # æ›´æ–°å½“å‰å®ä¾‹çš„æ•°æ®
        self.valid_pairs = filtered_pairs
        self.classes = stats['remaining_classes']
        self.annotations_dir = new_annotations_dir
        
        # é‡æ–°ç”Ÿæˆlabels.txt
        self._write_labels_file()
        
        # é‡æ–°åˆ’åˆ†æ•°æ®é›†
        self._split_dataset()
        
        return stats
    
    def check_and_fix_image_dimensions(self, auto_fix: bool = False):
        """
        æ£€æŸ¥XMLä¸­è®°å½•çš„å›¾åƒå°ºå¯¸ä¿¡æ¯æ˜¯å¦ä¸å®é™…å›¾åƒåŒ¹é…
        
        Args:
            auto_fix: æ˜¯å¦è‡ªåŠ¨ä¿®æ­£ä¸åŒ¹é…çš„ä¿¡æ¯
                     False: åªæ‰“å°è¯¦ç»†è­¦å‘Šä¿¡æ¯
                     True: ä¿®æ­£XMLæ•°æ®å¹¶è¦†ç›–3é€šé“å›¾ç‰‡åˆ°åŸå›¾ä¸Š
        
        Returns:
            dict: æ£€æŸ¥ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹æ£€æŸ¥å›¾åƒå°ºå¯¸ä¿¡æ¯ - è‡ªåŠ¨ä¿®æ­£: {auto_fix}")
        
        stats = {
            'total_checked': 0,
            'dimension_mismatches': 0,
            'channel_mismatches': 0,
            'read_errors': 0,
            'fixed_xmls': 0,
            'converted_images': 0,
            'mismatch_details': []
        }
        
        # æ¸…ç©ºä¹‹å‰çš„è®°å½•
        self.dimension_mismatches = []
        self.channel_mismatches = []
        
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(self.valid_pairs, desc="æ£€æŸ¥å›¾åƒå°ºå¯¸", unit="æ–‡ä»¶")
        
        for image_file, xml_file in pbar:
            try:
                # è¯»å–XMLä¸­çš„å°ºå¯¸ä¿¡æ¯
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                size_elem = root.find('size')
                if size_elem is None:
                    logger.warning(f"XMLæ–‡ä»¶ç¼ºå°‘sizeæ ‡ç­¾: {xml_file.name}")
                    continue
                
                # è·å–XMLä¸­è®°å½•çš„å°ºå¯¸
                xml_width = size_elem.find('width')
                xml_height = size_elem.find('height')
                xml_depth = size_elem.find('depth')
                
                if xml_width is None or xml_height is None or xml_depth is None:
                    logger.warning(f"XMLæ–‡ä»¶sizeæ ‡ç­¾ä¸å®Œæ•´: {xml_file.name}")
                    continue
                
                xml_w = int(xml_width.text)
                xml_h = int(xml_height.text)
                xml_d = int(xml_depth.text)
                
                # è¯»å–å®é™…å›¾åƒ
                img = cv2.imread(str(image_file))
                if img is None:
                    logger.error(f"æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {image_file.name}")
                    stats['read_errors'] += 1
                    continue
                
                # è·å–å®é™…å›¾åƒå°ºå¯¸
                actual_h, actual_w, actual_d = img.shape
                
                stats['total_checked'] += 1
                
                # æ£€æŸ¥å°ºå¯¸æ˜¯å¦åŒ¹é…
                dimension_mismatch = (xml_w != actual_w or xml_h != actual_h)
                channel_mismatch = (xml_d != actual_d or actual_d != 3)
                
                if dimension_mismatch or channel_mismatch:
                    mismatch_info = {
                        'xml_file': str(xml_file.absolute()),
                        'image_file': str(image_file.absolute()),
                        'xml_size': (xml_w, xml_h, xml_d),
                        'actual_size': (actual_w, actual_h, actual_d),
                        'dimension_mismatch': dimension_mismatch,
                        'channel_mismatch': channel_mismatch
                    }
                    stats['mismatch_details'].append(mismatch_info)
                    
                    if dimension_mismatch:
                        stats['dimension_mismatches'] += 1
                        self.dimension_mismatches.append(mismatch_info)
                        warning_msg = f"å°ºå¯¸ä¸åŒ¹é… - XML: {xml_file.absolute()}, å›¾åƒ: {image_file.absolute()}, XMLå°ºå¯¸({xml_w}x{xml_h}) vs å®é™…å°ºå¯¸({actual_w}x{actual_h})"
                        logger.warning(warning_msg)
                    
                    if channel_mismatch:
                        stats['channel_mismatches'] += 1
                        self.channel_mismatches.append(mismatch_info)
                        warning_msg = f"é€šé“æ•°ä¸åŒ¹é… - XML: {xml_file.absolute()}, å›¾åƒ: {image_file.absolute()}, XMLé€šé“({xml_d}) vs å®é™…é€šé“({actual_d})"
                        logger.warning(warning_msg)
                    
                    # å¦‚æœå¯ç”¨è‡ªåŠ¨ä¿®æ­£
                    if auto_fix:
                        # å¤„ç†å›¾åƒé€šé“æ•°
                        img_modified = False
                        if actual_d != 3:
                            if actual_d == 1:
                                # ç°åº¦å›¾è½¬RGB
                                img_fixed = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
                                img_modified = True
                            elif actual_d == 4:
                                # RGBAè½¬RGB
                                img_fixed = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)
                                img_modified = True
                            else:
                                logger.error(f"ä¸æ”¯æŒçš„é€šé“æ•°: {actual_d} - {image_file.name}")
                                continue
                            
                            if img_modified:
                                # è¦†ç›–åŸå›¾åƒ
                                cv2.imwrite(str(image_file), img_fixed)
                                logger.info(f"å·²è½¬æ¢å›¾åƒä¸º3é€šé“å¹¶è¦†ç›–åŸå›¾: {image_file.absolute()}")
                                stats['converted_images'] += 1
                                
                                # æ›´æ–°å®é™…å°ºå¯¸ä¿¡æ¯
                                actual_h, actual_w, actual_d = img_fixed.shape
                        
                        # ä¿®æ­£XMLä¸­çš„å°ºå¯¸ä¿¡æ¯
                        xml_width.text = str(actual_w)
                        xml_height.text = str(actual_h)
                        xml_depth.text = "3"  # å¼ºåˆ¶è®¾ä¸º3é€šé“
                        
                        # ä¿å­˜ä¿®æ­£åçš„XML
                        tree.write(xml_file, encoding=DEFAULT_ENCODING, xml_declaration=True)
                        logger.info(f"å·²ä¿®æ­£XMLå°ºå¯¸ä¿¡æ¯: {xml_file.absolute()} -> ({actual_w}x{actual_h}x3)")
                        stats['fixed_xmls'] += 1
                
            except Exception as e:
                logger.error(f"å¤„ç†å›¾åƒæ–‡ä»¶å¤±è´¥: {image_file.name} - {e}")
                stats['read_errors'] += 1
        
        # å…³é—­è¿›åº¦æ¡
        pbar.close()
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        logger.info(f"å›¾åƒå°ºå¯¸æ£€æŸ¥å®Œæˆ:")
        logger.info(f"  æ£€æŸ¥æ–‡ä»¶: {stats['total_checked']} ä¸ª")
        logger.info(f"  å°ºå¯¸ä¸åŒ¹é…: {stats['dimension_mismatches']} ä¸ª")
        logger.info(f"  é€šé“æ•°ä¸åŒ¹é…: {stats['channel_mismatches']} ä¸ª")
        logger.info(f"  è¯»å–é”™è¯¯: {stats['read_errors']} ä¸ª")
        
        if auto_fix:
            logger.info(f"  ä¿®æ­£XML: {stats['fixed_xmls']} ä¸ª")
            logger.info(f"  è½¬æ¢å›¾åƒ: {stats['converted_images']} ä¸ª")
        else:
            # å¦‚æœä¸è‡ªåŠ¨ä¿®æ­£ï¼Œæ‰“å°è¯¦ç»†çš„è­¦å‘Šä¿¡æ¯
            if stats['dimension_mismatches'] > 0 or stats['channel_mismatches'] > 0:
                logger.warning("å‘ç°å°ºå¯¸ä¸åŒ¹é…é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨ auto_fix=True å‚æ•°è‡ªåŠ¨ä¿®æ­£")
        
        return stats
    
    def _split_dataset(self):
        """æ•°æ®é›†åˆ’åˆ†åŠŸèƒ½"""
        logger.info("å¼€å§‹æ•°æ®é›†åˆ’åˆ†...")
        
        if not self.valid_pairs:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯¹ï¼Œè·³è¿‡æ•°æ®é›†åˆ’åˆ†")
            return
        
        # åˆ›å»ºImageSets/Mainç›®å½•
        self.imagesets_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰æœ‰æ•ˆæ–‡ä»¶çš„åŸºç¡€åç§°
        file_names = [pair[0].stem for pair in self.valid_pairs]
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        random.seed(RANDOM_SEED)
        random.shuffle(file_names)
        
        total_count = len(file_names)
        train_count = int(total_count * self.train_ratio)
        val_count = int(total_count * self.val_ratio)
        
        # åˆ’åˆ†æ•°æ®é›†
        train_files = file_names[:train_count]
        val_files = file_names[train_count:train_count + val_count]
        test_files = file_names[train_count + val_count:]
        
        # åˆ›å»ºtrainvalé›†åˆï¼ˆè®­ç»ƒé›†+éªŒè¯é›†ï¼‰
        trainval_files = train_files + val_files
        
        # å†™å…¥æ–‡ä»¶
        self._write_split_file(TRAIN_TXT, train_files)
        self._write_split_file(VAL_TXT, val_files)
        self._write_split_file(TEST_TXT, test_files)
        self._write_split_file(TRAINVAL_TXT, trainval_files)
        
        logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        logger.info(f"  è®­ç»ƒé›†: {len(train_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"  éªŒè¯é›†: {len(val_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"  æµ‹è¯•é›†: {len(test_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"  è®­ç»ƒéªŒè¯é›†: {len(trainval_files)} ä¸ªæ–‡ä»¶")
    
    def _write_split_file(self, filename: str, file_list: List[str]):
        """å†™å…¥åˆ’åˆ†æ–‡ä»¶"""
        file_path = self.imagesets_dir / filename
        
        try:
            with open(file_path, 'w', encoding=DEFAULT_ENCODING) as f:
                for file_name in file_list:
                    # æŸ¥æ‰¾å¯¹åº”çš„å›¾åƒæ–‡ä»¶ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                    image_file = None
                    for img_file, _ in self.valid_pairs:
                        if img_file.stem == file_name:
                            image_file = img_file
                            break
                    
                    if image_file:
                        # å†™å…¥æ ¼å¼: å›¾åƒè·¯å¾„\tæ ‡æ³¨è·¯å¾„
                        image_path = f"{JPEGS_DIR}/{image_file.name}"
                        annotation_path = f"{ANNOTATIONS_DIR}/{file_name}.xml"
                        f.write(f"{image_path}\t{annotation_path}\n")
            
            logger.debug(f"å†™å…¥åˆ’åˆ†æ–‡ä»¶: {filename} ({len(file_list)} ä¸ªæ–‡ä»¶)")
            
        except Exception as e:
            logger.error(f"å†™å…¥åˆ’åˆ†æ–‡ä»¶å¤±è´¥: {filename} - {e}")
    
    def get_classes(self) -> Set[str]:
        """è·å–æ‰€æœ‰ç±»åˆ«"""
        return self.classes.copy()
    
    def get_dataset_info(self):
        """è·å–æ•°æ®é›†åŸºæœ¬ä¿¡æ¯"""
        return {
            'dataset_name': self.dataset_name,
            'dataset_path': str(self.dataset_path.absolute()),
            'annotations_dir': str(self.annotations_dir),
            'total_valid_pairs': len(self.valid_pairs),
            'total_classes': len(self.classes),
            'classes': sorted(list(self.classes)),
            'missing_xml_count': len(self.missing_xml_files),
            'missing_image_count': len(self.missing_image_files),
            'missing_xml_files': [f.name for f in self.missing_xml_files],
            'missing_image_files': [f.name for f in self.missing_image_files],
            'split_ratios': {
                'train': self.train_ratio,
                'val': self.val_ratio,
                'test': self.test_ratio
            }
        }
    
    def print_summary(self):
        """æ‰“å°æ•°æ®é›†æ‘˜è¦ä¿¡æ¯"""
        logger.info("=== VOCæ•°æ®é›†æ‘˜è¦ ===")
        logger.info(f"æ•°æ®é›†åç§°: {self.dataset_name}")
        logger.info(f"æ•°æ®é›†è·¯å¾„: {self.dataset_path}")
        logger.info(f"æ ‡æ³¨ç›®å½•: {self.annotations_dir.name}")
        logger.info(f"æœ‰æ•ˆæ–‡ä»¶å¯¹: {len(self.valid_pairs)} ä¸ª")
        logger.info(f"ç±»åˆ«æ•°é‡: {len(self.classes)} ä¸ª")
        logger.info(f"ç±»åˆ«åˆ—è¡¨: {', '.join(sorted(self.classes))}")
        logger.info(f"ç¼ºå°‘XMLæ–‡ä»¶çš„å›¾åƒ: {len(self.missing_xml_files)} ä¸ª")
        logger.info(f"ç¼ºå°‘å›¾åƒæ–‡ä»¶çš„XML: {len(self.missing_image_files)} ä¸ª")
        logger.info(f"åˆ’åˆ†æ¯”ä¾‹: è®­ç»ƒé›†{self.train_ratio}, éªŒè¯é›†{self.val_ratio}, æµ‹è¯•é›†{self.test_ratio}")
        
        if self.missing_xml_files:
            logger.info("ç¼ºå°‘XMLçš„å›¾åƒæ–‡ä»¶:")
            for img_file in self.missing_xml_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"  {img_file.name}")
            if len(self.missing_xml_files) > 5:
                logger.info(f"  ... è¿˜æœ‰ {len(self.missing_xml_files) - 5} ä¸ª")

    def convert_to_coco_format(self, output_dir: str = None) -> Dict[str, str]:
        """
        å°†VOCæ ¼å¼æ•°æ®é›†è½¬æ¢ä¸ºCOCOæ ¼å¼
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºæ•°æ®é›†æ ¹ç›®å½•
            
        Returns:
            Dict[str, str]: ç”Ÿæˆçš„COCO JSONæ–‡ä»¶è·¯å¾„
            
        Raises:
            FileNotFoundError: å½“å¿…éœ€çš„æ–‡ä»¶ä¸å­˜åœ¨æ—¶
            ValueError: å½“æ•°æ®é›†æœªæ­£ç¡®åˆ’åˆ†æ—¶
        """
        import json
        from xml.etree import ElementTree as ET
        
        logger.info("å¼€å§‹è½¬æ¢VOCæ ¼å¼åˆ°COCOæ ¼å¼")
        
        # æ£€æŸ¥å¿…éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        imagesets_main_dir = self.dataset_path / IMAGESETS_DIR / MAIN_DIR
        labels_file = imagesets_main_dir / "labels.txt"
        
        if not labels_file.exists():
            raise FileNotFoundError(f"labels.txtæ–‡ä»¶ä¸å­˜åœ¨: {labels_file}")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = self.dataset_path
        else:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯»å–ç±»åˆ«ä¿¡æ¯
        with open(labels_file, 'r', encoding=DEFAULT_ENCODING) as f:
            label_lines = f.readlines()
            categories = [{'id': i + 1, 'name': label.strip()} for i, label in enumerate(label_lines)]
        
        logger.info(f"åŠ è½½äº† {len(categories)} ä¸ªç±»åˆ«")
        
        # åªè½¬æ¢æœ‰å†…å®¹çš„æ•°æ®é›†åˆ’åˆ†
        result_files = {}
        
        for split in ['train', 'val']:
            list_file = imagesets_main_dir / f"{split}.txt"
            if not list_file.exists():
                logger.warning(f"{split}.txt ä¸å­˜åœ¨ï¼Œè·³è¿‡ {split} é›†è½¬æ¢")
                continue
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å†…å®¹
            with open(list_file, 'r', encoding=DEFAULT_ENCODING) as f:
                lines = [line.strip() for line in f.readlines() if line.strip()]
            
            if not lines:
                logger.warning(f"{split}.txt æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡ {split} é›†è½¬æ¢")
                continue
                
            output_json = output_dir / f"{split}_coco.json"
            self._convert_split_to_coco(list_file, categories, output_json)
            result_files[split] = str(output_json)
            logger.info(f"{split} é›†è½¬æ¢å®Œæˆ: {output_json}")
        
        # æ³¨æ„ï¼šæ ¹æ®éœ€æ±‚ï¼ŒCOCOæ ¼å¼åªè½¬æ¢trainå’Œvalï¼Œä¸è½¬æ¢test
        # è¿™æ ·ç¡®ä¿COCOè¾“å‡ºä¸VOCçš„train.txtå’Œval.txtä¸€ä¸€å¯¹åº”
        test_list_file = imagesets_main_dir / "test.txt"
        if test_list_file.exists():
            logger.info("æ£€æµ‹åˆ°test.txtæ–‡ä»¶ï¼Œä½†COCOæ ¼å¼åªè½¬æ¢trainå’Œvalé›†")
        
        logger.info("VOCåˆ°COCOæ ¼å¼è½¬æ¢å®Œæˆ")
        return result_files
    
    def _convert_split_to_coco(self, list_file: Path, categories: List[Dict], output_json: Path):
        """
        è½¬æ¢å•ä¸ªæ•°æ®é›†åˆ’åˆ†åˆ°COCOæ ¼å¼
        
        Args:
            list_file: æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶è·¯å¾„
            categories: ç±»åˆ«ä¿¡æ¯åˆ—è¡¨
            output_json: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        """
        import json
        from xml.etree import ElementTree as ET
        
        images = []
        annotations = []
        annotation_id = 1
        
        # è¯»å–å›¾åƒåˆ—è¡¨
        with open(list_file, 'r', encoding=DEFAULT_ENCODING) as f:
            lines = f.readlines()
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            # è§£æå›¾åƒè·¯å¾„å’Œæ ‡æ³¨è·¯å¾„
            parts = line.split('\t')
            if len(parts) != 2:
                logger.warning(f"è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ: {line}")
                continue
                
            image_path, annotation_path = parts
            image_id = i + 1
            image_filename = Path(image_path).name
            
            # ä»XMLæ–‡ä»¶æå–å›¾åƒå°ºå¯¸
            xml_path = self.dataset_path / annotation_path
            if not xml_path.exists():
                logger.warning(f"XMLæ–‡ä»¶ä¸å­˜åœ¨: {xml_path}")
                continue
                
            try:
                tree = ET.parse(xml_path)
                root = tree.getroot()
                
                size_elem = root.find('size')
                if size_elem is None:
                    logger.warning(f"XMLæ–‡ä»¶ä¸­æ²¡æœ‰sizeä¿¡æ¯: {xml_path}")
                    continue
                    
                image_height = int(size_elem.find('height').text)
                image_width = int(size_elem.find('width').text)
                
                # æ·»åŠ å›¾åƒä¿¡æ¯
                images.append({
                    'id': image_id,
                    'file_name': image_filename,
                    'height': image_height,
                    'width': image_width,
                    'license': None,
                    'flickr_url': None,
                    'coco_url': None,
                    'date_captured': None
                })
                
                # è§£ææ ‡æ³¨ä¿¡æ¯
                objects = root.findall('object')
                for obj in objects:
                    # è·å–ç±»åˆ«åç§°
                    name_elem = obj.find('name')
                    if name_elem is None:
                        continue
                    label = name_elem.text
                    
                    # æŸ¥æ‰¾ç±»åˆ«ID
                    category_id = None
                    for cat in categories:
                        if cat['name'] == label:
                            category_id = cat['id']
                            break
                    
                    if category_id is None:
                        logger.warning(f"æœªçŸ¥ç±»åˆ« '{label}' åœ¨æ–‡ä»¶ {xml_path}")
                        continue
                    
                    # è·å–è¾¹ç•Œæ¡†
                    bbox_elem = obj.find('bndbox')
                    if bbox_elem is None:
                        continue
                        
                    xmin = int(bbox_elem.find('xmin').text)
                    ymin = int(bbox_elem.find('ymin').text)
                    xmax = int(bbox_elem.find('xmax').text)
                    ymax = int(bbox_elem.find('ymax').text)
                    
                    # è®¡ç®—COCOæ ¼å¼çš„è¾¹ç•Œæ¡† [x, y, width, height]
                    width = xmax - xmin
                    height = ymax - ymin
                    area = width * height
                    
                    # æ·»åŠ æ ‡æ³¨ä¿¡æ¯
                    annotations.append({
                        'id': annotation_id,
                        'image_id': image_id,
                        'category_id': category_id,
                        'bbox': [xmin, ymin, width, height],
                        'area': area,
                        'segmentation': [],
                        'iscrowd': 0
                    })
                    annotation_id += 1
                    
            except Exception as e:
                logger.error(f"å¤„ç†XMLæ–‡ä»¶æ—¶å‡ºé”™ {xml_path}: {e}")
                continue
        
        # æ„å»ºCOCOæ ¼å¼æ•°æ®
        coco_data = {
            'images': images,
            'annotations': annotations,
            'categories': categories
        }
        
        # ä¿å­˜JSONæ–‡ä»¶
        with open(output_json, 'w', encoding=DEFAULT_ENCODING) as f:
            json.dump(coco_data, f, indent=4, ensure_ascii=False)
        
        logger.info(f"è½¬æ¢å®Œæˆ: {len(images)} å¼ å›¾åƒ, {len(annotations)} ä¸ªæ ‡æ³¨")

    def one_click_process(self):
        """
        ä¸€é”®å®Œæˆè½¬æ¢å¹¶ä¿®å¤æ‰€æœ‰é—®é¢˜çš„å‡½æ•°
        
        åŒ…å«æ‰€æœ‰æ¸…æ´—æˆå‘˜å‡½æ•°ï¼Œä¿®æ­£å›¾ç‰‡å’ŒXMLæ–‡ä»¶ï¼Œåˆ’åˆ†VOCæ•°æ®é›†ï¼Œè½¬æ¢ä¸ºCOCOæ ¼å¼ã€‚
        è¿è¡Œå‰ä¼šæé†’ç”¨æˆ·æ˜¯å¦å·²å¤‡ä»½åŸå§‹æ•°æ®é›†ã€‚
        """
        logger.info("å¼€å§‹ä¸€é”®å¤„ç†æµç¨‹...")
        print("=== VOCæ•°æ®é›†ä¸€é”®å¤„ç† ===")
        print("âš ï¸  è­¦å‘Šï¼šæ­¤æ“ä½œå°†å¯¹æ•°æ®é›†è¿›è¡Œå…¨é¢å¤„ç†ï¼Œå¯èƒ½ä¼šä¿®æ”¹åŸå§‹æ–‡ä»¶ï¼")
        print("è¯·ç¡®ä¿æ‚¨å·²ç»å¤‡ä»½äº†åŸå§‹æ•°æ®é›†ã€‚")
        
        # ç”¨æˆ·ç¡®è®¤
        while True:
            user_input = input("\næ˜¯å¦å·²å¤‡ä»½åŸå§‹æ•°æ®é›†ï¼Ÿ(Y/N): ").strip().upper()
            if user_input == 'Y':
                logger.info("ç”¨æˆ·ç¡®è®¤å·²å¤‡ä»½æ•°æ®é›†ï¼Œå¼€å§‹å¤„ç†")
                print("å¼€å§‹ä¸€é”®å¤„ç†...")
                break
            elif user_input == 'N':
                logger.info("ç”¨æˆ·å–æ¶ˆå¤„ç†")
                print("å¤„ç†å·²å–æ¶ˆï¼Œè¯·å…ˆå¤‡ä»½æ•°æ®é›†åå†è¿è¡Œ")
                return
            else:
                print("è¯·è¾“å…¥ Y æˆ– N")
        
        try:
            # 1. æ£€æŸ¥å›¾åƒå°ºå¯¸å¹¶è‡ªåŠ¨ä¿®å¤
            logger.info("æ­¥éª¤1: æ£€æŸ¥å›¾åƒå°ºå¯¸...")
            print("ğŸ“‹ æ­¥éª¤1: æ£€æŸ¥å›¾åƒå°ºå¯¸...")
            self.check_and_fix_image_dimensions(auto_fix=True)
            
            # 2. é‡æ–°æå–ç±»åˆ«ï¼ˆå¯èƒ½æœ‰å˜åŒ–ï¼‰
            logger.info("æ­¥éª¤2: é‡æ–°æå–ç±»åˆ«...")
            print("ğŸ“‹ æ­¥éª¤2: é‡æ–°æå–ç±»åˆ«...")
            self.classes.clear()
            self._extract_classes()
            
            # 3. é‡æ–°åˆ’åˆ†æ•°æ®é›†
            logger.info("æ­¥éª¤3: é‡æ–°åˆ’åˆ†æ•°æ®é›†...")
            print("ğŸ“‹ æ­¥éª¤3: é‡æ–°åˆ’åˆ†æ•°æ®é›†...")
            self._split_dataset()
            
            # 4. è½¬æ¢ä¸ºCOCOæ ¼å¼
            logger.info("æ­¥éª¤4: è½¬æ¢ä¸ºCOCOæ ¼å¼...")
            print("ğŸ“‹ æ­¥éª¤4: è½¬æ¢ä¸ºCOCOæ ¼å¼...")
            self.convert_to_coco()
            
            # 5. ç”Ÿæˆç±»åˆ«ç»Ÿè®¡
            logger.info("æ­¥éª¤5: ç”Ÿæˆç±»åˆ«ç»Ÿè®¡...")
            print("ğŸ“‹ æ­¥éª¤5: ç”Ÿæˆç±»åˆ«ç»Ÿè®¡...")
            self.count_and_sort_classes()
            
            logger.info("=" * 60)
            logger.info("ä¸€é”®å¤„ç†å®Œæˆï¼")
            logger.info("=" * 60)
            print("\nâœ… ä¸€é”®å¤„ç†å®Œæˆï¼")
            print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"   - è®­ç»ƒé›†COCO: {self.dataset_path}/train_coco.json")
            print(f"   - éªŒè¯é›†COCO: {self.dataset_path}/val_coco.json")
            print(f"   - ç±»åˆ«æ ‡ç­¾: {self.dataset_path}/ImageSets/Main/labels.txt")
            print(f"   - ç±»åˆ«ç»Ÿè®¡: {self.dataset_path}/ImageSets/Main/count_all_cls.txt")
            
        except Exception as e:
            logger.error(f"ä¸€é”®å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            raise

    def remove_classes_only(self, exclude_classes: List[str] = None, new_annotations_suffix: str = "filtered") -> Dict:
        """
        ä¾¿åˆ©å‡½æ•°ï¼šä»…åˆ é™¤æŒ‡å®šç±»åˆ«å¹¶ç”Ÿæˆæ–°çš„Annotationsç›®å½•ï¼Œä¸æ‰§è¡Œåç»­æ¸…æ´—å’Œåˆ’åˆ†æµç¨‹
        
        Args:
            exclude_classes (List[str]): è¦åˆ é™¤çš„ç±»åˆ«åˆ—è¡¨ï¼Œé»˜è®¤åˆ é™¤['dragon fruit']
            new_annotations_suffix (str): æ–°Annotationsç›®å½•çš„åç¼€ï¼Œé»˜è®¤ä¸º"filtered"
            
        Returns:
            Dict: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        if exclude_classes is None:
            exclude_classes = ['dragon fruit']
            
        logger.info(f"å¼€å§‹åˆ é™¤ç±»åˆ«: {exclude_classes}")
        print(f"æ­£åœ¨åˆ é™¤ç±»åˆ«: {exclude_classes}")
        
        if not exclude_classes:
            logger.warning("æ²¡æœ‰æŒ‡å®šè¦åˆ é™¤çš„ç±»åˆ«")
            print("âš ï¸ æ²¡æœ‰æŒ‡å®šè¦åˆ é™¤çš„ç±»åˆ«")
            return {"success": False, "message": "æ²¡æœ‰æŒ‡å®šè¦åˆ é™¤çš„ç±»åˆ«"}
        
        try:
            # åˆ›å»ºæ–°çš„Annotationsç›®å½•
            new_annotations_dir = self.dataset_path / f"Annotations_{new_annotations_suffix}"
            new_annotations_dir.mkdir(exist_ok=True)
            
            processed_files = 0
            valid_files = 0
            empty_files = 0
            remaining_classes = set()
            
            # éå†æ‰€æœ‰XMLæ–‡ä»¶
            for xml_file in self.annotations_dir.glob("*.xml"):
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                # æ‰¾åˆ°æ‰€æœ‰objectå…ƒç´ 
                objects = root.findall('object')
                objects_to_remove = []
                
                # æ ‡è®°è¦åˆ é™¤çš„å¯¹è±¡
                for obj in objects:
                    name_elem = obj.find('name')
                    if name_elem is not None and name_elem.text in exclude_classes:
                        objects_to_remove.append(obj)
                
                # åˆ é™¤æ ‡è®°çš„å¯¹è±¡
                for obj in objects_to_remove:
                    root.remove(obj)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœ‰æ•ˆå¯¹è±¡
                remaining_objects = root.findall('object')
                if remaining_objects:
                    # æ”¶é›†å‰©ä½™ç±»åˆ«
                    for obj in remaining_objects:
                        name_elem = obj.find('name')
                        if name_elem is not None:
                            remaining_classes.add(name_elem.text)
                    
                    # ä¿å­˜ä¿®æ”¹åçš„XMLæ–‡ä»¶
                    new_xml_path = new_annotations_dir / xml_file.name
                    tree.write(new_xml_path, encoding='utf-8', xml_declaration=True)
                    valid_files += 1
                else:
                    empty_files += 1
                
                processed_files += 1
            
            result = {
                "success": True,
                "processed_files": processed_files,
                "valid_files": valid_files,
                "empty_files": empty_files,
                "new_annotations_dir": str(new_annotations_dir),
                "remaining_classes": sorted(list(remaining_classes)),
                "excluded_classes": exclude_classes
            }
            
            logger.info(f"ç±»åˆ«åˆ é™¤å®Œæˆ:")
            logger.info(f"  - å¤„ç†æ–‡ä»¶æ•°: {processed_files}")
            logger.info(f"  - æœ‰æ•ˆæ–‡ä»¶æ•°: {valid_files}")
            logger.info(f"  - ç©ºæ–‡ä»¶æ•°: {empty_files}")
            logger.info(f"  - æ–°ç›®å½•: {new_annotations_dir}")
            logger.info(f"  - å‰©ä½™ç±»åˆ«: {remaining_classes}")
            
            print(f"âœ… ç±»åˆ«åˆ é™¤å®Œæˆ!")
            print(f"   å¤„ç†æ–‡ä»¶æ•°: {processed_files}")
            print(f"   æœ‰æ•ˆæ–‡ä»¶æ•°: {valid_files}")
            print(f"   ç©ºæ–‡ä»¶æ•°: {empty_files}")
            print(f"   æ–°ç›®å½•: {new_annotations_dir}")
            print(f"   å‰©ä½™ç±»åˆ«: {sorted(list(remaining_classes))}")
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ é™¤ç±»åˆ«æ—¶å‡ºé”™: {e}")
            print(f"âŒ åˆ é™¤ç±»åˆ«å¤±è´¥: {e}")
            return {"success": False, "message": str(e)}

    def count_and_sort_classes(self, output_dir: str = None) -> Dict:
        """
        ç»Ÿè®¡æ‰€æœ‰ç±»åˆ«å¹¶æŒ‰è¦æ±‚æ’åºï¼Œè®°å½•æ¯ä¸ªç±»åˆ«çš„å‡ºç°æ¬¡æ•°
        
        Args:
            output_dir (str, optional): è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºImageSets/Mainç›®å½•
            
        Returns:
            Dict: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        from collections import Counter
        import re
        
        logger.info("å¼€å§‹ç»Ÿè®¡å’Œæ’åºç±»åˆ«...")
        print("æ­£åœ¨ç»Ÿè®¡å’Œæ’åºç±»åˆ«...")
        
        try:
            # è®¾ç½®è¾“å‡ºç›®å½•
            if output_dir is None:
                output_dir = self.dataset_path / "ImageSets" / "Main"
            else:
                output_dir = Path(output_dir)
            
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç»Ÿè®¡æ‰€æœ‰ç±»åˆ«åŠå…¶å‡ºç°æ¬¡æ•°
            class_counter = Counter()
            processed_files = 0
            
            # éå†æ‰€æœ‰XMLæ–‡ä»¶ç»Ÿè®¡ç±»åˆ«
            for xml_file in self.annotations_dir.glob("*.xml"):
                try:
                    tree = ET.parse(xml_file)
                    root = tree.getroot()
                    
                    # ç»Ÿè®¡è¯¥æ–‡ä»¶ä¸­çš„æ‰€æœ‰å¯¹è±¡
                    for obj in root.findall('object'):
                        name_elem = obj.find('name')
                        if name_elem is not None and name_elem.text:
                            class_counter[name_elem.text] += 1
                    
                    processed_files += 1
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†XMLæ–‡ä»¶å¤±è´¥: {xml_file} - {e}")
                    continue
            
            if not class_counter:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç±»åˆ«")
                print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç±»åˆ«")
                return {"success": False, "message": "æœªæ‰¾åˆ°ä»»ä½•ç±»åˆ«"}
            
            # è‡ªå®šä¹‰æ’åºå‡½æ•°ï¼šä¼˜å…ˆæ•°å­—å¼€å¤´ï¼Œç„¶åå­—æ¯ï¼Œä»å°åˆ°å¤§
            def sort_key(class_name):
                # å¦‚æœä»¥æ•°å­—å¼€å¤´ï¼Œè¿”å›(0, æ•°å­—å€¼, å‰©ä½™å­—ç¬¦ä¸²)
                if class_name and class_name[0].isdigit():
                    # æå–å¼€å¤´çš„æ•°å­—
                    match = re.match(r'^(\d+)', class_name)
                    if match:
                        num = int(match.group(1))
                        remaining = class_name[len(match.group(1)):]
                        return (0, num, remaining.lower())
                
                # å¦‚æœä»¥å­—æ¯å¼€å¤´ï¼Œè¿”å›(1, 0, å®Œæ•´å­—ç¬¦ä¸²)
                return (1, 0, class_name.lower())
            
            # æŒ‰è‡ªå®šä¹‰è§„åˆ™æ’åº
            sorted_classes = sorted(class_counter.keys(), key=sort_key)
            
            # å†™å…¥labels.txtæ–‡ä»¶
            labels_file = output_dir / "labels.txt"
            with open(labels_file, 'w', encoding='utf-8') as f:
                for class_name in sorted_classes:
                    f.write(f"{class_name}\n")
            
            # å†™å…¥count_all_cls.txtæ–‡ä»¶
            count_file = output_dir / "count_all_cls.txt"
            with open(count_file, 'w', encoding='utf-8') as f:
                f.write("ç±»åˆ«åç§°\tå‡ºç°æ¬¡æ•°\n")
                f.write("-" * 30 + "\n")
                for class_name in sorted_classes:
                    count = class_counter[class_name]
                    f.write(f"{class_name}\t{count}\n")
            
            # å‡†å¤‡ç»“æœ
            result = {
                "success": True,
                "processed_files": processed_files,
                "total_classes": len(sorted_classes),
                "sorted_classes": sorted_classes,
                "class_counts": dict(class_counter),
                "labels_file": str(labels_file),
                "count_file": str(count_file),
                "total_annotations": sum(class_counter.values())
            }
            
            logger.info(f"ç±»åˆ«ç»Ÿè®¡å®Œæˆ:")
            logger.info(f"  - å¤„ç†æ–‡ä»¶æ•°: {processed_files}")
            logger.info(f"  - ç±»åˆ«æ€»æ•°: {len(sorted_classes)}")
            logger.info(f"  - æ ‡æ³¨æ€»æ•°: {sum(class_counter.values())}")
            logger.info(f"  - æ’åºåç±»åˆ«: {sorted_classes}")
            logger.info(f"  - labelsæ–‡ä»¶: {labels_file}")
            logger.info(f"  - ç»Ÿè®¡æ–‡ä»¶: {count_file}")
            
            print(f"âœ… ç±»åˆ«ç»Ÿè®¡å®Œæˆ!")
            print(f"   å¤„ç†æ–‡ä»¶æ•°: {processed_files}")
            print(f"   ç±»åˆ«æ€»æ•°: {len(sorted_classes)}")
            print(f"   æ ‡æ³¨æ€»æ•°: {sum(class_counter.values())}")
            print(f"   æ’åºåç±»åˆ«: {sorted_classes}")
            print(f"   labelsæ–‡ä»¶: {labels_file}")
            print(f"   ç»Ÿè®¡æ–‡ä»¶: {count_file}")
            
            return result
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡ç±»åˆ«æ—¶å‡ºé”™: {e}")
            print(f"âŒ ç»Ÿè®¡ç±»åˆ«å¤±è´¥: {e}")
            return {"success": False, "message": str(e)}

    def check_classes_in_annotations(self, annotations_path: str = None, target_classes: List[str] = None) -> Dict:
        """
        æ£€æŸ¥æŒ‡å®šæ ‡æ³¨æ–‡ä»¶å¤¹ä¸­æ˜¯å¦è¿˜å­˜åœ¨ç›®æ ‡ç±»åˆ«
        
        Args:
            annotations_path (str, optional): æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸ºNoneä½¿ç”¨æˆå‘˜å˜é‡ä¸­çš„æ•°æ®é›†è·¯å¾„
            target_classes (List[str], optional): è¦æ£€æŸ¥çš„ç›®æ ‡ç±»åˆ«åˆ—è¡¨ï¼Œé»˜è®¤ä¸º['dragon fruit']
            
        Returns:
            Dict: åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        if target_classes is None:
            target_classes = ['dragon fruit']
            
        # è®¾ç½®æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„
        if annotations_path is None:
            check_annotations_dir = self.annotations_dir
            path_source = "æˆå‘˜å˜é‡è·¯å¾„"
        else:
            check_annotations_dir = Path(annotations_path)
            path_source = "ä¼ å…¥è·¯å¾„"
            
        logger.info(f"å¼€å§‹æ£€æŸ¥ç±»åˆ«: {target_classes}")
        logger.info(f"æ£€æŸ¥è·¯å¾„: {check_annotations_dir} (æ¥æº: {path_source})")
        print(f"ğŸ” æ£€æŸ¥ç±»åˆ«: {target_classes}")
        print(f"ğŸ“ æ£€æŸ¥è·¯å¾„: {check_annotations_dir} (æ¥æº: {path_source})")
        
        if not check_annotations_dir.exists():
            error_msg = f"æ ‡æ³¨æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {check_annotations_dir}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            return {"success": False, "message": error_msg}
        
        try:
            # ç»Ÿè®¡ä¿¡æ¯
            total_files = 0
            files_with_target_classes = 0
            target_class_occurrences = {cls: 0 for cls in target_classes}
            all_found_classes = set()
            files_containing_targets = []
            
            # éå†æ‰€æœ‰XMLæ–‡ä»¶
            xml_files = list(check_annotations_dir.glob("*.xml"))
            if not xml_files:
                warning_msg = f"åœ¨ {check_annotations_dir} ä¸­æœªæ‰¾åˆ°XMLæ–‡ä»¶"
                logger.warning(warning_msg)
                print(f"âš ï¸ {warning_msg}")
                return {"success": False, "message": warning_msg}
            
            for xml_file in xml_files:
                try:
                    tree = ET.parse(xml_file)
                    root = tree.getroot()
                    
                    file_has_target = False
                    file_classes = set()
                    
                    # æ£€æŸ¥è¯¥æ–‡ä»¶ä¸­çš„æ‰€æœ‰å¯¹è±¡
                    for obj in root.findall('object'):
                        name_elem = obj.find('name')
                        if name_elem is not None and name_elem.text:
                            class_name = name_elem.text
                            all_found_classes.add(class_name)
                            file_classes.add(class_name)
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡ç±»åˆ«
                            if class_name in target_classes:
                                target_class_occurrences[class_name] += 1
                                file_has_target = True
                    
                    if file_has_target:
                        files_with_target_classes += 1
                        files_containing_targets.append({
                            'file': xml_file.name,
                            'classes': list(file_classes.intersection(set(target_classes)))
                        })
                    
                    total_files += 1
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†XMLæ–‡ä»¶å¤±è´¥: {xml_file} - {e}")
                    continue
            
            # å‡†å¤‡ç»“æœ
            has_target_classes = any(count > 0 for count in target_class_occurrences.values())
            total_target_occurrences = sum(target_class_occurrences.values())
            
            result = {
                "success": True,
                "annotations_path": str(check_annotations_dir),
                "path_source": path_source,
                "target_classes": target_classes,
                "total_files_checked": total_files,
                "files_with_target_classes": files_with_target_classes,
                "has_target_classes": has_target_classes,
                "target_class_occurrences": target_class_occurrences,
                "total_target_occurrences": total_target_occurrences,
                "all_found_classes": sorted(list(all_found_classes)),
                "files_containing_targets": files_containing_targets
            }
            
            # è¾“å‡ºç»“æœ
            logger.info(f"ç±»åˆ«æ£€æŸ¥å®Œæˆ:")
            logger.info(f"  - æ£€æŸ¥æ–‡ä»¶æ•°: {total_files}")
            logger.info(f"  - åŒ…å«ç›®æ ‡ç±»åˆ«çš„æ–‡ä»¶æ•°: {files_with_target_classes}")
            logger.info(f"  - æ˜¯å¦å­˜åœ¨ç›®æ ‡ç±»åˆ«: {has_target_classes}")
            logger.info(f"  - ç›®æ ‡ç±»åˆ«å‡ºç°æ¬¡æ•°: {target_class_occurrences}")
            logger.info(f"  - æ€»ç›®æ ‡ç±»åˆ«å‡ºç°æ¬¡æ•°: {total_target_occurrences}")
            logger.info(f"  - æ‰€æœ‰å‘ç°çš„ç±»åˆ«: {sorted(list(all_found_classes))}")
            
            print(f"âœ… ç±»åˆ«æ£€æŸ¥å®Œæˆ!")
            print(f"   æ£€æŸ¥æ–‡ä»¶æ•°: {total_files}")
            print(f"   åŒ…å«ç›®æ ‡ç±»åˆ«çš„æ–‡ä»¶æ•°: {files_with_target_classes}")
            print(f"   ç›®æ ‡ç±»åˆ«å‡ºç°æ¬¡æ•°: {target_class_occurrences}")
            print(f"   æ€»ç›®æ ‡ç±»åˆ«å‡ºç°æ¬¡æ•°: {total_target_occurrences}")
            print(f"   æ‰€æœ‰å‘ç°çš„ç±»åˆ«: {sorted(list(all_found_classes))}")
            
            if has_target_classes:
                print(f"âš ï¸ è­¦å‘Š: ä»ç„¶å­˜åœ¨ç›®æ ‡ç±»åˆ«!")
                print(f"   åŒ…å«ç›®æ ‡ç±»åˆ«çš„æ–‡ä»¶:")
                for file_info in files_containing_targets:
                    print(f"     - {file_info['file']}: {file_info['classes']}")
            else:
                print(f"ğŸ‰ æˆåŠŸ: ç›®æ ‡ç±»åˆ«å·²å®Œå…¨åˆ é™¤!")
            
            return result
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç±»åˆ«æ—¶å‡ºé”™: {e}")
            print(f"âŒ æ£€æŸ¥ç±»åˆ«å¤±è´¥: {e}")
            return {"success": False, "message": str(e)}

if __name__ == "__main__":
    # æµ‹è¯•VOCæ•°æ®é›†ç±»
    dataset_path = "./dataset/Fruit"
    
    try:
        voc_dataset = VOCDataset(dataset_path)
        voc_dataset.print_summary()
        
        # æµ‹è¯•å›¾åƒå°ºå¯¸æ£€æŸ¥åŠŸèƒ½
        print(f"\n=== æµ‹è¯•å›¾åƒå°ºå¯¸æ£€æŸ¥åŠŸèƒ½ ===")
        check_stats = voc_dataset.check_and_fix_image_dimensions(auto_fix=False)
        print(f"æ£€æŸ¥ç»Ÿè®¡: {check_stats}")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯: {e}")